{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 12106625,
          "sourceType": "datasetVersion",
          "datasetId": 7622110
        }
      ],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "49141409",
      "cell_type": "code",
      "source": [
        "!pip install -U \"numpy<2.0\" \"matplotlib==3.8.2\" scikit-learn pandas \"transformers==4.44.2\" seaborn tqdm \"torch==2.4.0\" --force-reinstall --no-cache-dir"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T19:01:28.439256Z",
          "iopub.execute_input": "2025-06-18T19:01:28.439540Z",
          "iopub.status.idle": "2025-06-18T19:05:05.309755Z",
          "shell.execute_reply.started": "2025-06-18T19:01:28.439513Z",
          "shell.execute_reply": "2025-06-18T19:05:05.308984Z"
        },
        "id": "49141409"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b1476ba5",
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Force single GPU usage\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T19:05:12.955124Z",
          "iopub.execute_input": "2025-06-18T19:05:12.955772Z",
          "iopub.status.idle": "2025-06-18T19:05:19.119916Z",
          "shell.execute_reply.started": "2025-06-18T19:05:12.955737Z",
          "shell.execute_reply": "2025-06-18T19:05:19.119280Z"
        },
        "id": "b1476ba5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2450f8ab",
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"google/flan-t5-xl\"\n",
        "\n",
        "# Model configuration for inference\n",
        "MAX_LENGTH = 2048\n",
        "GENERATION_MAX_LENGTH = 10\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Adjust this path to point to your local dataset location\n",
        "TRAIN_DATA_PATH = \"/content/Dataset/train_preprocessed.csv\"\n",
        "OUTPUT_DIR = \"/content/Output/\"\n",
        "TEXT_COLUMN = \"comment_text\"\n",
        "LABEL_COLUMNS = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "NUM_LABELS = len(LABEL_COLUMNS)\n",
        "\n",
        "# Defining comprehensive few-shot prompts and descriptions for each toxicity category\n",
        "PROMPT_TEMPLATES = {\n",
        "    \"toxic\": \"\"\"A toxic comment is a rude, disrespectful, or unreasonable utterance that is likely to make people leave a discussion.\n",
        "\n",
        "Examples of toxic comments:\n",
        "1. \"Your opinion is completely off-base and pointless—you don't bring anything of value to this conversation.\"\n",
        "2. \"Honestly, I can't believe you're still typing—this is just mind-numbing fluff.\"\n",
        "3. \"This entire thread is embarrassing; you clearly don't know what you're talking about.\"\n",
        "4. \"Can you please stop wasting everyone's time with this garbage?\"\n",
        "5. \"Reading your posts is exhausting; maybe you should rethink posting at all.\"\n",
        "\n",
        "Based on the definition and examples above, is this comment toxic? Answer yes or no.\n",
        "Comment: {text}\"\"\",\n",
        "\n",
        "    \"severe_toxic\": \"\"\"A severe_toxic comment exhibits very hateful or aggressive language—far beyond mere profanity—such that it is highly likely to drive participants away or cause them to abandon sharing their perspective.\n",
        "\n",
        "Examples of severely toxic comments:\n",
        "1. \"You are an absolute disgrace—no one wants to hear your toxic drivel; just disappear.\"\n",
        "2. \"I sincerely hope you get banned and never show your pathetic face here again.\"\n",
        "3. \"Your comments are the vile rubbish of the internet, and I want you gone.\"\n",
        "4. \"Nobody cares about your warped opinions, and you'd be doing us all a favor shutting up forever.\"\n",
        "5. \"This is filth, pure filth—and it's poisoning everyone's time; stop posting or get banned.\"\n",
        "\n",
        "Based on the definition and examples above, is this comment severely toxic? Answer yes or no.\n",
        "Comment: {text}\"\"\",\n",
        "\n",
        "    \"obscene\": \"\"\"An obscene comment contains vulgar or profane language (e.g., explicit curse words or sexual references) intended to shock or offend.\n",
        "\n",
        "Examples of obscene comments:\n",
        "1. \"You're a fucking clown, and your posts are full of utter bullshit.\"\n",
        "2. \"Go fuck yourself—you don't have a clue what you're talking about.\"\n",
        "3. \"That was some pathetic horny rambling—get out of here with that smut.\"\n",
        "4. \"Your comment is pure disgusting crap—full of stupid fucking lies.\"\n",
        "5. \"You piece of shit, stop spewing your obscene drivel at us.\"\n",
        "\n",
        "Based on the definition and examples above, is this comment obscene? Answer yes or no.\n",
        "Comment: {text}\"\"\",\n",
        "\n",
        "    \"threat\": \"\"\"A threat is a statement expressing an intention to inflict pain, injury, damage, or other hostile action against an individual or group, or to coerce them by implying future harm.\n",
        "\n",
        "Examples of threatening comments:\n",
        "1. \"If you don't shut up, I'm going to make sure you regret it.\"\n",
        "2. \"Try posting that again and I'll find out where you live.\"\n",
        "3. \"You better delete that now or you'll pay the price.\"\n",
        "4. \"Keep talking like that and I'll come after you personally.\"\n",
        "5. \"One more smart comment from you and I guarantee you'll be sorry.\"\n",
        "\n",
        "Based on the definition and examples above, does this comment contain a threat? Answer yes or no.\n",
        "Comment: {text}\"\"\",\n",
        "\n",
        "    \"insult\": \"\"\"An insult is an abusive, inflammatory, or negative remark directed at a person or group, intended to demean, belittle, or provoke.\n",
        "\n",
        "Examples of insulting comments:\n",
        "1. \"You're the most clueless person I've seen online—embarrassing.\"\n",
        "2. \"Nice rant, genius—you'd fail a kindergarten test.\"\n",
        "3. \"That idea was so stupid I can't believe you even wrote it.\"\n",
        "4. \"You're pathetic—someone as ignorant as you shouldn't be posting in public.\"\n",
        "5. \"Thanks for your insight, Captain Obvious—you're really helpful (not).\"\n",
        "\n",
        "Based on the definition and examples above, is this comment insulting? Answer yes or no.\n",
        "Comment: {text}\"\"\",\n",
        "\n",
        "    \"identity_hate\": \"\"\"An identity_hate comment targets a person or group based on a protected characteristic (e.g., race, religion, gender identity, sexual orientation), conveying negative or hateful sentiments toward that identity.\n",
        "\n",
        "Examples of identity-based hate comments:\n",
        "1. \"Of course you'd say that—that's typical of your religion, always acting like that.\"\n",
        "2. \"People from your country are worthless and should be ashamed.\"\n",
        "3. \"Why are gays always so annoying? Just go away.\"\n",
        "4. \"You're only smart for a woman—too bad you're just a girl.\"\n",
        "5. \"I can't stand black people—they ruin everything they touch.\"\n",
        "\n",
        "Based on the definition and examples above, does this comment contain identity-based hate? Answer yes or no.\n",
        "Comment: {text}\"\"\"\n",
        "}\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Label columns: {LABEL_COLUMNS}\")\n",
        "print(f\"Number of labels: {NUM_LABELS}\")\n",
        "print(f\"Max input length: {MAX_LENGTH} tokens\")\n",
        "print(f\"Batch size reduced to: {BATCH_SIZE} for XL model\")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T20:31:33.992623Z",
          "iopub.execute_input": "2025-06-18T20:31:33.992929Z",
          "iopub.status.idle": "2025-06-18T20:31:34.001889Z",
          "shell.execute_reply.started": "2025-06-18T20:31:33.992909Z",
          "shell.execute_reply": "2025-06-18T20:31:34.001088Z"
        },
        "id": "2450f8ab"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4af6a3e0-fedf-4dc6-a879-a6e9f8ac3cd2",
      "cell_type": "code",
      "source": [
        "# Load and Split Dataset into Train/Validation/Test\n",
        "print(\"Loading preprocessed training data...\")\n",
        "try:\n",
        "    full_train_df = pd.read_csv(TRAIN_DATA_PATH)\n",
        "    print(f\" Training data loaded successfully: {full_train_df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\" Training data not found at: {TRAIN_DATA_PATH}\")\n",
        "    print(\"Please update paths in the configuration section\")\n",
        "    raise\n",
        "\n",
        "# Displaying basic information about the data\n",
        "print(\"\\nOriginal training data columns:\")\n",
        "print(full_train_df.columns.tolist())\n",
        "\n",
        "# Check if required columns exist\n",
        "required_columns = [TEXT_COLUMN] + LABEL_COLUMNS\n",
        "missing_columns = [col for col in required_columns if col not in full_train_df.columns]\n",
        "\n",
        "if missing_columns:\n",
        "    print(f\"\\n Missing required columns in training data: {missing_columns}\")\n",
        "    print(f\"Available columns: {full_train_df.columns.tolist()}\")\n",
        "    print(\"\\nPlease ensure your preprocessed data has the following columns:\")\n",
        "    print(f\"- {TEXT_COLUMN} (the processed text)\")\n",
        "    print(f\"- {', '.join(LABEL_COLUMNS)} (label columns)\")\n",
        "    raise ValueError(\"Missing required columns\")\n",
        "else:\n",
        "    print(\"\\n All required columns found in training data\")\n",
        "\n",
        "print(\"\\nFirst few rows of original training data:\")\n",
        "print(full_train_df.head())\n",
        "\n",
        "# Check label distribution in original data\n",
        "print(\"\\nLabel distribution in original training data:\")\n",
        "label_stats = full_train_df[LABEL_COLUMNS].sum()\n",
        "print(label_stats)\n",
        "\n",
        "# Calculate percentage of positive labels\n",
        "print(\"\\nPercentage of positive labels:\")\n",
        "label_percentages = (full_train_df[LABEL_COLUMNS].sum() / len(full_train_df)) * 100\n",
        "print(label_percentages)\n",
        "\n",
        "# Visualize original label distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "label_stats.plot(kind='bar')\n",
        "plt.title('Label Counts in Original Training Data')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "label_percentages.plot(kind='bar')\n",
        "plt.title('Label Percentages in Original Training Data')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Percentage (%)')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Split the data into train/validation/test (70%/15%/15%)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SPLITTING DATA INTO TRAIN/VALIDATION/TEST\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# First split: separate test set (15% of total data)\n",
        "train_val_df, test_df = train_test_split(\n",
        "    full_train_df,\n",
        "    test_size=0.15,\n",
        "    random_state=42,\n",
        "    stratify=full_train_df[LABEL_COLUMNS[0]]  # Stratify on primary toxic label\n",
        ")\n",
        "\n",
        "# Second split: separate validation from remaining data (15% of total = ~17.6% of remaining)\n",
        "train_df, val_df = train_test_split(\n",
        "    train_val_df,\n",
        "    test_size=0.176,  # This gives us ~15% of original data for validation\n",
        "    random_state=42,\n",
        "    stratify=train_val_df[LABEL_COLUMNS[0]]  # Stratify on primary toxic label\n",
        ")\n",
        "\n",
        "print(f\"Data split completed:\")\n",
        "print(f\"- Training set: {len(train_df):,} samples ({len(train_df)/len(full_train_df)*100:.1f}%)\")\n",
        "print(f\"- Validation set: {len(val_df):,} samples ({len(val_df)/len(full_train_df)*100:.1f}%)\")\n",
        "print(f\"- Test set: {len(test_df):,} samples ({len(test_df)/len(full_train_df)*100:.1f}%)\")\n",
        "print(f\"- Total: {len(train_df) + len(val_df) + len(test_df):,} samples\")\n",
        "\n",
        "print(f\"\\nUsing full test set for evaluation with Flan-T5 XL model\")\n",
        "\n",
        "# Create test set without labels for prediction\n",
        "test_df_no_labels = test_df[[TEXT_COLUMN]].copy()\n",
        "if 'id' not in test_df.columns:\n",
        "    # Create an ID column if it doesn't exist\n",
        "    test_df_no_labels['id'] = range(len(test_df_no_labels))\n",
        "    test_df['id'] = range(len(test_df))\n",
        "else:\n",
        "    test_df_no_labels['id'] = test_df['id'].copy()\n",
        "\n",
        "# Save the split datasets\n",
        "train_split_path = f\"{OUTPUT_DIR}/train_split.csv\"\n",
        "val_split_path = f\"{OUTPUT_DIR}/val_split.csv\"\n",
        "test_split_with_labels_path = f\"{OUTPUT_DIR}/test_split_with_labels.csv\"\n",
        "test_split_no_labels_path = f\"{OUTPUT_DIR}/test_split_no_labels.csv\"\n",
        "\n",
        "train_df.to_csv(train_split_path, index=False)\n",
        "val_df.to_csv(val_split_path, index=False)\n",
        "test_df.to_csv(test_split_with_labels_path, index=False)\n",
        "test_df_no_labels.to_csv(test_split_no_labels_path, index=False)\n",
        "\n",
        "print(f\"\\n Split datasets saved:\")\n",
        "print(f\"- Training data: {train_split_path}\")\n",
        "print(f\"- Validation data: {val_split_path}\")\n",
        "print(f\"- Test data (with labels): {test_split_with_labels_path}\")\n",
        "print(f\"- Test data (without labels): {test_split_no_labels_path}\")\n",
        "\n",
        "# Compare label distributions across splits\n",
        "print(f\"\\n LABEL DISTRIBUTION COMPARISON:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "splits_info = {\n",
        "    'Original': full_train_df,\n",
        "    'Train': train_df,\n",
        "    'Validation': val_df,\n",
        "    'Test': test_df\n",
        "}\n",
        "\n",
        "comparison_data = []\n",
        "for split_name, split_df in splits_info.items():\n",
        "    row = {'Split': split_name, 'Size': len(split_df)}\n",
        "    for label in LABEL_COLUMNS:\n",
        "        count = split_df[label].sum()\n",
        "        percentage = (count / len(split_df)) * 100\n",
        "        row[f'{label}_count'] = count\n",
        "        row[f'{label}_pct'] = percentage\n",
        "    comparison_data.append(row)\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"\\nLabel counts and percentages by split:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualize distribution comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Label Distribution Comparison Across Splits', fontsize=16)\n",
        "\n",
        "# Plot 1: Sample counts\n",
        "splits = ['Train', 'Validation', 'Test']\n",
        "sizes = [len(train_df), len(val_df), len(test_df)]\n",
        "axes[0, 0].bar(splits, sizes, color=['skyblue', 'lightgreen', 'lightcoral'])\n",
        "axes[0, 0].set_title('Sample Counts by Split')\n",
        "axes[0, 0].set_ylabel('Number of Samples')\n",
        "for i, v in enumerate(sizes):\n",
        "    axes[0, 0].text(i, v + max(sizes)*0.01, f'{v:,}', ha='center', va='bottom')\n",
        "\n",
        "# Plot 2: Label percentages for each split\n",
        "label_pcts = {\n",
        "    'Train': [(train_df[label].sum() / len(train_df)) * 100 for label in LABEL_COLUMNS],\n",
        "    'Validation': [(val_df[label].sum() / len(val_df)) * 100 for label in LABEL_COLUMNS],\n",
        "    'Test': [(test_df[label].sum() / len(test_df)) * 100 for label in LABEL_COLUMNS]\n",
        "}\n",
        "\n",
        "x = np.arange(len(LABEL_COLUMNS))\n",
        "width = 0.25\n",
        "\n",
        "axes[0, 1].bar(x - width, label_pcts['Train'], width, label='Train', color='skyblue')\n",
        "axes[0, 1].bar(x, label_pcts['Validation'], width, label='Validation', color='lightgreen')\n",
        "axes[0, 1].bar(x + width, label_pcts['Test'], width, label='Test', color='lightcoral')\n",
        "\n",
        "axes[0, 1].set_title('Label Percentages by Split')\n",
        "axes[0, 1].set_xlabel('Labels')\n",
        "axes[0, 1].set_ylabel('Percentage (%)')\n",
        "axes[0, 1].set_xticks(x)\n",
        "axes[0, 1].set_xticklabels([label.replace('_', '\\n') for label in LABEL_COLUMNS], rotation=45)\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# Plot 3: Training set label distribution (bar chart)\n",
        "train_label_counts = [train_df[label].sum() for label in LABEL_COLUMNS]\n",
        "axes[1, 0].bar(range(len(LABEL_COLUMNS)), train_label_counts, color='skyblue')\n",
        "axes[1, 0].set_title('Training Set Label Counts')\n",
        "axes[1, 0].set_xlabel('Labels')\n",
        "axes[1, 0].set_ylabel('Count')\n",
        "axes[1, 0].set_xticks(range(len(LABEL_COLUMNS)))\n",
        "axes[1, 0].set_xticklabels([label.replace('_', '\\n') for label in LABEL_COLUMNS], rotation=45)\n",
        "\n",
        "# Plot 4: Test set label distribution (bar chart)\n",
        "test_label_counts = [test_df[label].sum() for label in LABEL_COLUMNS]\n",
        "axes[1, 1].bar(range(len(LABEL_COLUMNS)), test_label_counts, color='lightcoral')\n",
        "axes[1, 1].set_title('Test Set Label Counts')\n",
        "axes[1, 1].set_xlabel('Labels')\n",
        "axes[1, 1].set_ylabel('Count')\n",
        "axes[1, 1].set_xticks(range(len(LABEL_COLUMNS)))\n",
        "axes[1, 1].set_xticklabels([label.replace('_', '\\n') for label in LABEL_COLUMNS], rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUTPUT_DIR}/data_split_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Update paths for the inference process\n",
        "print(f\"\\n Updating paths for inference process...\")\n",
        "TRAIN_DATA_PATH = train_split_path\n",
        "VAL_DATA_PATH = val_split_path\n",
        "TEST_DATA_PATH = test_split_no_labels_path  # Use the version without labels for prediction\n",
        "TEST_WITH_LABELS_PATH = test_split_with_labels_path  # Keep reference to version with labels\n",
        "\n",
        "print(f\"Updated paths:\")\n",
        "print(f\"- TRAIN_DATA_PATH: {TRAIN_DATA_PATH}\")\n",
        "print(f\"- VAL_DATA_PATH: {VAL_DATA_PATH}\")\n",
        "print(f\"- TEST_DATA_PATH: {TEST_DATA_PATH}\")\n",
        "print(f\"- TEST_WITH_LABELS_PATH: {TEST_WITH_LABELS_PATH}\")\n",
        "\n",
        "# Use the split data for inference\n",
        "train_df_split = train_df.copy()\n",
        "val_df_split = val_df.copy()\n",
        "test_df = test_df_no_labels.copy()  # This will be used for prediction\n",
        "\n",
        "print(f\"\\n Data splitting completed successfully!\")\n",
        "print(f\"Final dataset sizes:\")\n",
        "print(f\"- Training: {len(train_df_split):,} samples\")\n",
        "print(f\"- Validation: {len(val_df_split):,} samples\")\n",
        "print(f\"- Test: {len(test_df):,} samples\")\n",
        "print(f\"Ready to proceed with inference on the full test data.\")\n",
        "print(f\"Note: Using full test set will provide more comprehensive evaluation results.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T20:04:35.043640Z",
          "iopub.execute_input": "2025-06-18T20:04:35.043951Z",
          "iopub.status.idle": "2025-06-18T20:04:43.408226Z",
          "shell.execute_reply.started": "2025-06-18T20:04:35.043926Z",
          "shell.execute_reply": "2025-06-18T20:04:43.407552Z"
        },
        "id": "4af6a3e0-fedf-4dc6-a879-a6e9f8ac3cd2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "10fb9538",
      "cell_type": "code",
      "source": [
        "# Loading Flan-T5 XL Model and Tokenizer\n",
        "print(\"Loading Flan-T5 XL model and tokenizer...\")\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(\"This may take several minutes due to the model size...\")\n",
        "\n",
        "# Clear any existing CUDA cache\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "print(f\"Tokenizer loaded. Vocab size: {len(tokenizer)}\")\n",
        "\n",
        "# Load model with optimized settings for XL\n",
        "try:\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "        low_cpu_mem_usage=True,  # Optimize CPU memory usage\n",
        "    )\n",
        "\n",
        "    if not torch.cuda.is_available():\n",
        "        model = model.to(device)\n",
        "\n",
        "    print(f\"Model loaded successfully!\")\n",
        "    print(f\"Model loaded on device: {next(model.parameters()).device}\")\n",
        "    print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
        "\n",
        "    # Check GPU memory usage if CUDA is available\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "        print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"This might be due to insufficient memory. Consider:\")\n",
        "    print(\"1. Using a smaller model (flan-t5-large or flan-t5-base)\")\n",
        "    print(\"2. Using CPU-only inference (slower but less memory)\")\n",
        "    print(\"3. Increasing system RAM or GPU memory\")\n",
        "    raise\n",
        "\n",
        "# Testing the model with a simple example\n",
        "print(\"\\nTesting model with a simple example...\")\n",
        "test_prompt = \"Is this comment toxic? Answer yes or no.\\nComment: Hello world\"\n",
        "test_inputs = tokenizer(test_prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH)\n",
        "if torch.cuda.is_available():\n",
        "    test_inputs = {k: v.to(model.device) for k, v in test_inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_outputs = model.generate(\n",
        "        **test_inputs,\n",
        "        max_new_tokens=GENERATION_MAX_LENGTH,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    test_response = tokenizer.decode(test_outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Test prompt: {test_prompt}\")\n",
        "    print(f\"Test response: {test_response}\")\n",
        "\n",
        "print(\"\\nFlan-T5 XL model loaded successfully and ready for inference!\")\n",
        "print(\"Note: Inference will be slower than the base model but potentially more accurate.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T19:06:01.300588Z",
          "iopub.execute_input": "2025-06-18T19:06:01.301331Z",
          "iopub.status.idle": "2025-06-18T19:13:22.504085Z",
          "shell.execute_reply.started": "2025-06-18T19:06:01.301303Z",
          "shell.execute_reply": "2025-06-18T19:13:22.503282Z"
        },
        "id": "10fb9538"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ef020b79-9445-4e57-9593-f500c137b21b",
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Generating Predictions using Flan-T5 XL with Fixed Structure\n",
        "print(\"Generating predictions on test set using Flan-T5 XL with corrected inference structure...\")\n",
        "print(\"Processing all 6 labels together for each sample to match ground truth structure.\")\n",
        "\n",
        "# Load test data from the split\n",
        "print(f\"Loading test data from: {TEST_DATA_PATH}\")\n",
        "test_df = pd.read_csv(TEST_DATA_PATH)\n",
        "print(f\"Test data loaded: {test_df.shape}\")\n",
        "\n",
        "def parse_response(response):\n",
        "    \"\"\"Parse model response to binary prediction\"\"\"\n",
        "    response = response.lower().strip()\n",
        "    if 'yes' in response:\n",
        "        return 1\n",
        "    elif 'no' in response:\n",
        "        return 0\n",
        "    else:\n",
        "        # Default to 0 if response is unclear\n",
        "        return 0\n",
        "\n",
        "def predict_sample_all_labels(text, batch_size=6):\n",
        "    \"\"\"Generate predictions for all 6 labels for a single sample\"\"\"\n",
        "    # Truncate text if too long to avoid memory issues\n",
        "    if len(text) > 800:\n",
        "        text = text[:800] + \"...\"\n",
        "\n",
        "    # Prepare prompts for all 6 labels\n",
        "    prompts = []\n",
        "    for label in LABEL_COLUMNS:\n",
        "        prompt = PROMPT_TEMPLATES[label].format(text=text)\n",
        "        prompts.append(prompt)\n",
        "\n",
        "    # Tokenize all prompts together\n",
        "    inputs = tokenizer(\n",
        "        prompts,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate responses for all labels\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=GENERATION_MAX_LENGTH,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            num_beams=1,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    # Decode responses: T5 generate() already returns only generated tokens\n",
        "    responses = []\n",
        "    for output in outputs:\n",
        "        response = tokenizer.decode(output, skip_special_tokens=True)\n",
        "        responses.append(response)\n",
        "\n",
        "    # Parse responses to binary predictions\n",
        "    predictions = [parse_response(response) for response in responses]\n",
        "    return predictions\n",
        "\n",
        "def predict_batch_all_labels(texts, batch_size=4):\n",
        "    \"\"\"Process a batch of samples, each getting all 6 label predictions\"\"\"\n",
        "    batch_predictions = []\n",
        "\n",
        "    for text in texts:\n",
        "        try:\n",
        "            sample_predictions = predict_sample_all_labels(text)\n",
        "            batch_predictions.append(sample_predictions)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing sample: {e}\")\n",
        "            # Default to all zeros for failed samples\n",
        "            batch_predictions.append([0] * len(LABEL_COLUMNS))\n",
        "            if torch.cuda.is_available() and \"memory\" in str(e).lower():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    return batch_predictions\n",
        "\n",
        "# Optimize batch size based on available GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    allocated_memory = torch.cuda.memory_allocated() / 1024**3\n",
        "    available_memory = total_memory - allocated_memory\n",
        "\n",
        "    print(f\"GPU Memory Status:\")\n",
        "    print(f\"  Total: {total_memory:.1f} GB\")\n",
        "    print(f\"  Allocated: {allocated_memory:.1f} GB\")\n",
        "    print(f\"  Available: {available_memory:.1f} GB\")\n",
        "\n",
        "    # Conservative batch size for XL model\n",
        "    if available_memory > 8:\n",
        "        SAMPLE_BATCH_SIZE = 64\n",
        "    elif available_memory > 4:\n",
        "        SAMPLE_BATCH_SIZE = 4\n",
        "    else:\n",
        "        SAMPLE_BATCH_SIZE = 2\n",
        "else:\n",
        "    SAMPLE_BATCH_SIZE = 2\n",
        "\n",
        "print(f\"Using sample batch size: {SAMPLE_BATCH_SIZE}\")\n",
        "\n",
        "# Initialize predictions array\n",
        "num_samples = len(test_df)\n",
        "num_labels = len(LABEL_COLUMNS)\n",
        "predictions = np.zeros((num_samples, num_labels), dtype=int)\n",
        "\n",
        "# Prepare all texts\n",
        "all_texts = test_df[TEXT_COLUMN].tolist()\n",
        "\n",
        "print(f\"Starting corrected prediction for {num_samples} samples across {num_labels} categories...\")\n",
        "print(f\"Processing structure: Each sample gets predictions for ALL labels together\")\n",
        "print(f\"Using sample batch size: {SAMPLE_BATCH_SIZE}\")\n",
        "\n",
        "# Calculate estimated time\n",
        "total_batches = len(all_texts) / SAMPLE_BATCH_SIZE\n",
        "estimated_minutes = total_batches * 0.5  # Rough estimate: 0.5 minutes per batch\n",
        "print(f\"Estimated time: ~{estimated_minutes:.1f} minutes\")\n",
        "\n",
        "# Memory management: Clear cache before starting\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Process samples in batches\n",
        "processed_samples = 0\n",
        "for i in range(0, num_samples, SAMPLE_BATCH_SIZE):\n",
        "    batch_end = min(i + SAMPLE_BATCH_SIZE, num_samples)\n",
        "    batch_texts = all_texts[i:batch_end]\n",
        "\n",
        "    print(f\"Processing samples {i+1}-{batch_end} of {num_samples}...\")\n",
        "\n",
        "    try:\n",
        "        # Get predictions for this batch\n",
        "        batch_predictions = predict_batch_all_labels(batch_texts, SAMPLE_BATCH_SIZE)\n",
        "\n",
        "        # Store predictions\n",
        "        for j, sample_preds in enumerate(batch_predictions):\n",
        "            sample_idx = i + j\n",
        "            if sample_idx < num_samples:\n",
        "                predictions[sample_idx, :] = sample_preds\n",
        "                processed_samples += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in batch {i//SAMPLE_BATCH_SIZE + 1}: {e}\")\n",
        "        # Fill with zeros for failed batch\n",
        "        for j in range(len(batch_texts)):\n",
        "            sample_idx = i + j\n",
        "            if sample_idx < num_samples:\n",
        "                predictions[sample_idx, :] = [0] * num_labels\n",
        "                processed_samples += 1\n",
        "\n",
        "        # Clear CUDA cache on memory errors\n",
        "        if torch.cuda.is_available() and \"memory\" in str(e).lower():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Progress update every 10 batches\n",
        "    if (i // SAMPLE_BATCH_SIZE + 1) % 10 == 0:\n",
        "        elapsed_time = time.time() - start_time\n",
        "        samples_per_second = processed_samples / elapsed_time\n",
        "        remaining_samples = num_samples - processed_samples\n",
        "        estimated_remaining = remaining_samples / samples_per_second / 60 if samples_per_second > 0 else 0\n",
        "        print(f\"  Progress: {processed_samples}/{num_samples} samples ({processed_samples/num_samples*100:.1f}%). \"\n",
        "              f\"Speed: {samples_per_second:.1f} samples/sec. ETA: {estimated_remaining:.1f} min\")\n",
        "\n",
        "    # Memory cleanup every few batches\n",
        "    if torch.cuda.is_available() and (i // SAMPLE_BATCH_SIZE + 1) % 5 == 0:\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nCorrected prediction completed in {total_time/60:.1f} minutes!\")\n",
        "print(f\"Processed samples: {processed_samples}/{num_samples}\")\n",
        "print(f\"Predictions shape: {predictions.shape}\")\n",
        "print(f\"Predictions are binary: {np.all(np.isin(predictions, [0, 1]))}\")\n",
        "\n",
        "# Verify we processed all samples\n",
        "if processed_samples != num_samples:\n",
        "    print(f\"WARNING: Only processed {processed_samples} out of {num_samples} samples!\")\n",
        "else:\n",
        "    print(f\"SUCCESS: Processed all {num_samples} samples!\")\n",
        "\n",
        "# Create submission DataFrame\n",
        "submission_df = test_df[['id']].copy()\n",
        "for i, label in enumerate(LABEL_COLUMNS):\n",
        "    submission_df[label] = predictions[:, i]\n",
        "\n",
        "# Save predictions\n",
        "submission_filename = f\"{OUTPUT_DIR}/test_predictions_binary_xl_fixed.csv\"\n",
        "submission_df.to_csv(submission_filename, index=False)\n",
        "print(f\"Fixed binary test predictions saved to {submission_filename}\")\n",
        "\n",
        "# Show sample predictions\n",
        "print(\"\\nSample fixed binary test predictions:\")\n",
        "print(submission_df.head(10))\n",
        "\n",
        "# Show prediction statistics\n",
        "print(\"\\nFixed Prediction Statistics:\")\n",
        "for i, label in enumerate(LABEL_COLUMNS):\n",
        "    positive_count = np.sum(predictions[:, i])\n",
        "    positive_percentage = (positive_count / num_samples) * 100\n",
        "    print(f\"{label.replace('_', ' ').title():<15}: {positive_count:4d} positive ({positive_percentage:.1f}%)\")\n",
        "\n",
        "print(f\"\\nFixed prediction generation with Flan-T5 XL completed successfully!\")\n",
        "print(f\"Total time: {total_time/60:.1f} minutes\")\n",
        "print(f\"Average time per sample: {total_time/num_samples:.3f} seconds\")\n",
        "print(f\"Structure: Each sample processed for all {num_labels} labels together\")\n",
        "print(f\"Throughput: {processed_samples/total_time:.1f} samples per second\")\n",
        "\n",
        "# Final memory cleanup\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"\\nFinal GPU memory usage:\")\n",
        "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-19T03:43:34.484648Z",
          "iopub.execute_input": "2025-06-19T03:43:34.485261Z"
        },
        "id": "ef020b79-9445-4e57-9593-f500c137b21b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "74a4310b",
      "cell_type": "code",
      "source": [
        "# Evaluate on Split Test Set with Ground Truth Labels\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATION ON SPLIT TEST SET WITH GROUND TRUTH LABELS (FLAN-T5 XL)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load the test set with labels for evaluation\n",
        "print(f\" Loading test set with labels from: {TEST_WITH_LABELS_PATH}\")\n",
        "test_with_labels_df = pd.read_csv(TEST_WITH_LABELS_PATH)\n",
        "print(f\" Test set with labels loaded: {test_with_labels_df.shape}\")\n",
        "\n",
        "# Load binary predictions from XL model\n",
        "predictions_file_path = f\"{OUTPUT_DIR}/test_predictions_binary_xl.csv\"\n",
        "\n",
        "if os.path.exists(predictions_file_path):\n",
        "    print(f\" Loading binary predictions from: {predictions_file_path}\")\n",
        "    pred_df = pd.read_csv(predictions_file_path)\n",
        "    print(f\" Binary predictions loaded: {pred_df.shape}\")\n",
        "\n",
        "    # Ensure both datasets have the same length and order\n",
        "    if len(test_with_labels_df) == len(pred_df):\n",
        "        # Extract true labels and binary predictions\n",
        "        y_true = test_with_labels_df[LABEL_COLUMNS].values.astype(int)\n",
        "        y_pred_binary = pred_df[LABEL_COLUMNS].values.astype(int)\n",
        "\n",
        "        print(f\" Evaluation data shape: {y_true.shape}\")\n",
        "        print(f\" Predictions are binary: {np.all(np.isin(y_pred_binary, [0, 1]))}\")\n",
        "        print(f\" Labels are binary: {np.all(np.isin(y_true, [0, 1]))}\")\n",
        "\n",
        "        # Calculate comprehensive metrics\n",
        "        print(\"\\n SPLIT TEST SET EVALUATION RESULTS (FLAN-T5 XL):\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # Per-label metrics\n",
        "        print(\"Per-Label Performance:\")\n",
        "        for i, label in enumerate(LABEL_COLUMNS):\n",
        "            true_labels = y_true[:, i]\n",
        "            pred_labels = y_pred_binary[:, i]\n",
        "\n",
        "            # Basic metrics\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "                true_labels, pred_labels, average='binary', zero_division=0\n",
        "            )\n",
        "\n",
        "            # Accuracy for this label\n",
        "            accuracy = np.mean(true_labels == pred_labels)\n",
        "\n",
        "            # Support\n",
        "            support = np.sum(true_labels)\n",
        "\n",
        "            print(f\"{label.replace('_', ' ').title():<15}: P={precision:.3f} R={recall:.3f} F1={f1:.3f} Acc={accuracy:.3f} (Support: {support})\")\n",
        "\n",
        "        # Aggregate metrics\n",
        "        print(f\"\\n AGGREGATE PERFORMANCE:\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Macro averages\n",
        "        macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
        "            y_true, y_pred_binary, average='macro', zero_division=0\n",
        "        )\n",
        "\n",
        "        # Micro averages\n",
        "        micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(\n",
        "            y_true.flatten(), y_pred_binary.flatten(), average='micro', zero_division=0\n",
        "        )\n",
        "\n",
        "        # Exact match accuracy (all labels must be correct)\n",
        "        exact_match = np.mean(np.all(y_pred_binary == y_true, axis=1))\n",
        "\n",
        "        # Hamming loss\n",
        "        hamming_loss = np.mean(y_pred_binary != y_true)\n",
        "\n",
        "        # Label-wise accuracy\n",
        "        label_accuracies = [np.mean(y_true[:, i] == y_pred_binary[:, i]) for i in range(len(LABEL_COLUMNS))]\n",
        "        mean_label_accuracy = np.mean(label_accuracies)\n",
        "\n",
        "        print(f\"Macro Average    : P={macro_precision:.3f} R={macro_recall:.3f} F1={macro_f1:.3f}\")\n",
        "        print(f\"Micro Average    : P={micro_precision:.3f} R={micro_recall:.3f} F1={micro_f1:.3f}\")\n",
        "        print(f\"Exact Match Acc  : {exact_match:.3f}\")\n",
        "        print(f\"Mean Label Acc   : {mean_label_accuracy:.3f}\")\n",
        "        print(f\"Hamming Loss     : {hamming_loss:.3f}\")\n",
        "\n",
        "        print(f\"\\n Flan-T5 XL binary evaluation completed successfully!\")\n",
        "        print(f\" Exact Match Accuracy: {exact_match:.3f}\")\n",
        "        print(f\" Macro F1 Score: {macro_f1:.3f}\")\n",
        "        print(f\" Evaluated on {len(y_true)} test samples\")\n",
        "        print(f\" Model used: {MODEL_NAME}\")\n",
        "\n",
        "    else:\n",
        "        print(f\" Length mismatch: Test labels ({len(test_with_labels_df)}) vs Predictions ({len(pred_df)})\")\n",
        "else:\n",
        "    print(f\" Binary predictions file not found: {predictions_file_path}\")\n",
        "    print(f\" Make sure to run the prediction cell first with the XL model.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-19T03:00:37.121431Z",
          "iopub.execute_input": "2025-06-19T03:00:37.121707Z",
          "iopub.status.idle": "2025-06-19T03:00:37.329781Z",
          "shell.execute_reply.started": "2025-06-19T03:00:37.121684Z",
          "shell.execute_reply": "2025-06-19T03:00:37.329162Z"
        },
        "id": "74a4310b"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}