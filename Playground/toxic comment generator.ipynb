{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44194a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3512e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 11:27:23.457305: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748942843.503620   23195 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748942843.519404   23195 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748942843.619380   23195 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748942843.619402   23195 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748942843.619404   23195 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748942843.619405   23195 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-03 11:27:23.631811: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset shape: (159571, 12)\n",
      "Sample data:\n",
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
      "0             0        0       0       0              0   \n",
      "1             0        0       0       0              0   \n",
      "\n",
      "                                      processed_text  original_length  \\\n",
      "0  explanation edits made my username hardcore me...              264   \n",
      "1  daww ! he match background colour im seemingly...              112   \n",
      "\n",
      "   processed_length  length_reduction  \n",
      "0               202         23.484848  \n",
      "1                86         23.214286  \n",
      "Created dataset with 159571 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f28c8a1e6a4a0589665d369b014479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/159571 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 159571\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23195/2192023697.py:85: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:42, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.856500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.480700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.119500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.508500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.887900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.600100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.442600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.244400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.470800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, pipeline\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset\n",
    "train_data = pd.read_csv('../Dataset/train_preprocessed.csv')\n",
    "print(f\"Dataset shape: {train_data.shape}\")\n",
    "print(f\"Sample data:\\n{train_data.head(2)}\")\n",
    "\n",
    "# Check if any required columns are missing\n",
    "required_columns = ['comment_text'] + ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "missing_columns = [col for col in required_columns if col not in train_data.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing columns in dataset: {missing_columns}\")\n",
    "\n",
    "# Format data\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "def format_labels(row):\n",
    "    return ', '.join([f\"{col}={int(row[col])}\" for col in label_cols])  # Ensure integers\n",
    "\n",
    "# Handle potential NaNs\n",
    "train_data['comment_text'] = train_data['comment_text'].fillna(\"\")\n",
    "for col in label_cols:\n",
    "    train_data[col] = train_data[col].fillna(0).astype(int)\n",
    "\n",
    "# Create formatted text\n",
    "train_data['input'] = train_data.apply(format_labels, axis=1)\n",
    "train_data['output'] = train_data['comment_text']\n",
    "train_data['text'] = \"<toxicity> \" + train_data['input'] + \" </toxicity> <comment> \" + train_data['output']\n",
    "\n",
    "# Convert to dataset\n",
    "dataset = Dataset.from_pandas(train_data[['text']])\n",
    "print(f\"Created dataset with {len(dataset)} examples\")\n",
    "\n",
    "# Tokenize with the right format for causal language modeling\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize inputs\n",
    "    result = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256  # Increased max length\n",
    "    )\n",
    "    \n",
    "    # Set up labels for language modeling (same as input_ids)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text']  # Remove text column as it's not needed after tokenization\n",
    ")\n",
    "print(f\"Tokenized dataset: {tokenized_dataset}\")\n",
    "\n",
    "# Configure training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-toxic\",\n",
    "    per_device_train_batch_size=3,  # Reduced batch size\n",
    "    gradient_accumulation_steps=4,  # Add gradient accumulation\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=10,\n",
    "    max_steps=100,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    fp16=torch.cuda.is_available(),  # Only use fp16 if GPU is available\n",
    ")\n",
    "\n",
    "# Load model and configure trainer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))  # Resize for any special tokens\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train\n",
    "try:\n",
    "    trainer.train()\n",
    "    # Save model\n",
    "    model.save_pretrained(\"./gpt2-toxic-final\")\n",
    "    tokenizer.save_pretrained(\"./gpt2-toxic-final\")\n",
    "    print(\"Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ff42200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<toxicity> toxic=1, obscene=1, insult=1 </toxicity> <comment> It is also important to note that the first sentence of the above sentence is not intended to mean that the sentence is not a reference to the current definition of \"toxic\". The only reference to that definition is to the fact that the definition of \"toxic\" is based on a definition that is very clearly defined and is very similar to the definition of \"toxic\" and \"toxic\" in the law. It is important to note that the reference to \"toxic\" was in the original law, as well as the new definition of \"toxic\" and \"toxic\" was inserted to make it more clear that it is not a reference to the current definition. This does not mean that \"toxic\" is not a reference to any specific crime, but that is simply a reference to a specific crime. The question is whether the statute is used in the context of an \"attempt to harm\" or \"attempt to injure\". The statute does not directly refer to any specific crime or to any specific crime. The statute does not directly refer to any specific crime, but does refer to a specific crime and its nature. The statute specifically refers to the crime of assault in the first instance, and to the crime of murder in the first instance. The\n"
     ]
    }
   ],
   "source": [
    "# Reload model if needed\n",
    "# from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"./gpt2-toxic\")\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"<toxicity> toxic=1, obscene=1, insult=1 </toxicity> <comment>\"\n",
    "output = text_generator(prompt, max_length=50, do_sample=True, top_k=50, top_p=0.95)\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc0d7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<toxicity>  obscene=1 </toxicity> <comment> No, I don't think you're saying you don't want to kill?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"<toxicity>  obscene=1 </toxicity> <comment>\"\n",
    "output = text_generator(prompt, max_length=25, do_sample=True, top_k=50, top_p=0.95)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f06d214f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<toxicity> toxic=1, obscene=1, insult=1 </toxicity> <comment> \"As you can see, this is the worst case for what I would recommend to my friend, I was just a bit too young for this. And my friend is a teenager and I am not sure if I understand what you are saying. But I believe it is worth considering as a possibility. My friend's age, I have never experienced a person I was so young for in this world, and I am aware that it is a very dangerous place to be. That is why I have decided to ask you to take a look at the story of my friend's death. I will be taking it very seriously. I am sure that there is some truth in it. I will take you to the death of my friend. I am sure that the last part of it will be written by the last person who will be alive, that is because of the same person who died in the last few days. The last person who will be alive will be my friend and I am sure that it is not possible to get the information out of this case. I am going to be very careful to tell you what I am doing and what I am doing and when I am done with it. I will only be doing so if you will please.\"\n"
     ]
    }
   ],
   "source": [
    "# Reload model if needed\n",
    "# from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"./gpt2-toxic\")\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"<toxicity> toxic=1, obscene=1, insult=1 </toxicity> <comment>\"\n",
    "output = text_generator(prompt, max_length=50, do_sample=True, top_k=50, top_p=0.95)\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e19c1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=56) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<toxicity> toxic=1, obscene=1, insult=1 </toxicity> <comment> The way to describe this is to say \"You know how I feel? I think I'm not the only one who feels this way. I'd rather have a lot of people that are on the same side as you, but I think it's better to have a lot of people that you can be friends with, but that's it. It's not that different than having someone who is not on the same side as you. You know that, you know that too, you know that. I think that's a good thing. I think I would like to see people who are on the same side as you, but I think we should be able to come together, but I don't want that to be the case. You know, we should be able to do things together. I want to see people who are not on the same side as you, but we should be able to be friends. We should be able to do things together. If we are not able to do that, I think we should be able to do things.\"\n"
     ]
    }
   ],
   "source": [
    "# Example prompt\n",
    "prompt = \"<toxicity> toxic=1, obscene=1, insult=1 </toxicity> <comment>\"\n",
    "output = text_generator(prompt, max_length=56, do_sample=True, top_k=50, top_p=0.95)\n",
    "print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
