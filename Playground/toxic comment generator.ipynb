{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44194a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3512e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 11:18:32.289228: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748510312.334539  125175 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748510312.349082  125175 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748510312.441545  125175 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748510312.441576  125175 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748510312.441578  125175 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748510312.441580  125175 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-29 11:18:32.453945: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset shape: (159571, 9)\n",
      "Sample data:\n",
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
      "0             0        0       0       0              0   \n",
      "1             0        0       0       0              0   \n",
      "\n",
      "                                      processed_text  \n",
      "0  explanation edits made username hardcore metal...  \n",
      "1  daww match background colour im seemingly stuc...  \n",
      "Created dataset with 159571 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e09ef2eea242e59ae6bf44ed68f411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/159571 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 159571\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_125175/2192023697.py:85: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:04, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.856500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.480700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.687300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.119800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.508900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.888100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.600100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.442500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.244500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.470700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, pipeline\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset\n",
    "train_data = pd.read_csv('../Dataset/train_preprocessed.csv')\n",
    "print(f\"Dataset shape: {train_data.shape}\")\n",
    "print(f\"Sample data:\\n{train_data.head(2)}\")\n",
    "\n",
    "# Check if any required columns are missing\n",
    "required_columns = ['comment_text'] + ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "missing_columns = [col for col in required_columns if col not in train_data.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing columns in dataset: {missing_columns}\")\n",
    "\n",
    "# Format data\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "def format_labels(row):\n",
    "    return ', '.join([f\"{col}={int(row[col])}\" for col in label_cols])  # Ensure integers\n",
    "\n",
    "# Handle potential NaNs\n",
    "train_data['comment_text'] = train_data['comment_text'].fillna(\"\")\n",
    "for col in label_cols:\n",
    "    train_data[col] = train_data[col].fillna(0).astype(int)\n",
    "\n",
    "# Create formatted text\n",
    "train_data['input'] = train_data.apply(format_labels, axis=1)\n",
    "train_data['output'] = train_data['comment_text']\n",
    "train_data['text'] = \"<toxicity> \" + train_data['input'] + \" </toxicity> <comment> \" + train_data['output']\n",
    "\n",
    "# Convert to dataset\n",
    "dataset = Dataset.from_pandas(train_data[['text']])\n",
    "print(f\"Created dataset with {len(dataset)} examples\")\n",
    "\n",
    "# Tokenize with the right format for causal language modeling\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize inputs\n",
    "    result = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256  # Increased max length\n",
    "    )\n",
    "    \n",
    "    # Set up labels for language modeling (same as input_ids)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['text']  # Remove text column as it's not needed after tokenization\n",
    ")\n",
    "print(f\"Tokenized dataset: {tokenized_dataset}\")\n",
    "\n",
    "# Configure training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-toxic\",\n",
    "    per_device_train_batch_size=3,  # Reduced batch size\n",
    "    gradient_accumulation_steps=4,  # Add gradient accumulation\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=10,\n",
    "    max_steps=100,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    fp16=torch.cuda.is_available(),  # Only use fp16 if GPU is available\n",
    ")\n",
    "\n",
    "# Load model and configure trainer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))  # Resize for any special tokens\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train\n",
    "try:\n",
    "    trainer.train()\n",
    "    # Save model\n",
    "    model.save_pretrained(\"./gpt2-toxic-final\")\n",
    "    tokenizer.save_pretrained(\"./gpt2-toxic-final\")\n",
    "    print(\"Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ff42200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<toxicity> toxic=1, obscene=1, insult=1 </toxicity> <comment> It is also important to note that the first sentence of the above sentence is not intended to mean that the sentence is not a reference to the current definition of \"toxic\". The only reference to that definition is to the fact that the definition of \"toxic\" is based on a definition that is very clearly defined and is very similar to the definition of \"toxic\" and \"toxic\" in the law. It is important to note that the reference to \"toxic\" was in the original law, as well as the new definition of \"toxic\" and \"toxic\" was inserted to make it more clear that it is not a reference to the current definition. This does not mean that \"toxic\" is not a reference to any specific crime, but that it is a reference to a specific crime. The question is whether the statute is used in the context of an \"attempt to harm\" or \"attempt to injure\". The statute does not provide that the statute is a reference to a crime. It is a reference to a crime that is not a crime, and the statute does not mean that the person is guilty of any crime. The statute also does not say that crime is a crime. It does not mean that the person is guilty\n"
     ]
    }
   ],
   "source": [
    "# Reload model if needed\n",
    "# from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"./gpt2-toxic\")\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"<toxicity> toxic=1, obscene=1, insult=1 </toxicity> <comment>\"\n",
    "output = text_generator(prompt, max_length=50, do_sample=True, top_k=50, top_p=0.95)\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cc0d7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<toxicity>  obscene=1 </toxicity> <comment> \"The word'sexism' is a euphemism for 'disrespect' or contempt. I think it's used more often in the context of the word'sexism' than in the way it is used in the context of'sexism.' \"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"<toxicity>  obscene=1 </toxicity> <comment>\"\n",
    "output = text_generator(prompt, max_length=25, do_sample=True, top_k=50, top_p=0.95)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f06d214f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<toxicity> toxic=1, obscene=1, insult=1 </toxicity> <comment> No, I don't think you're a troll, but I have some ideas for you, and you might as well. You're a good person, and I'm not too sure I understand.\n"
     ]
    }
   ],
   "source": [
    "# Reload model if needed\n",
    "# from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"./gpt2-toxic\")\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"<toxicity> toxic=1, obscene=1, insult=1 </toxicity> <comment>\"\n",
    "output = text_generator(prompt, max_length=50, do_sample=True, top_k=50, top_p=0.95)\n",
    "print(output[0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
