{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Text Vectorization for Toxic Comment Classification\n",
    "\n",
    "This notebook compares traditional vectorization methods (TF-IDF, Count) with advanced embedding techniques (Word2Vec, GloVe, FastText) on the toxic comment classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import LinearSVC\n",
    "import nltk\n",
    "import time\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPU cores: 16\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "n_cores = multiprocessing.cpu_count()\n",
    "print(f\"Available CPU cores: {n_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (159571, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>daww match background colour im seemingly stuc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man im really not trying edit war guy cons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cant make real suggestion improvement wondered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sir hero chance remember page thats</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                      processed_text  \n",
       "0  explanation edits made username hardcore metal...  \n",
       "1  daww match background colour im seemingly stuc...  \n",
       "2  hey man im really not trying edit war guy cons...  \n",
       "3  cant make real suggestion improvement wondered...  \n",
       "4                sir hero chance remember page thats  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load preprocessed data\n",
    "train_data = pd.read_csv('../Dataset/train_preprocessed.csv')\n",
    "\n",
    "# Check the data\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 127656\n",
      "Validation set size: 31915\n"
     ]
    }
   ],
   "source": [
    "# Define the features and target labels\n",
    "X = train_data['processed_text']  # Use the preprocessed text\n",
    "y = train_data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "# Handle missing values\n",
    "X = X.fillna(\"\")  # Replace NaN values with empty strings\n",
    "y = y.fillna(0)   # Replace any missing target values with 0\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y['toxic']\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Advanced Vectorizers\n",
    "\n",
    "Let's implement custom vectorizers for Word2Vec, GloVe, and FastText embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Vectorizer\n",
    "class Word2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=100, window=5, min_count=1, workers=4, sg=1):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.sg = sg  # 0 for CBOW, 1 for Skip-gram\n",
    "        self.model = None\n",
    "        self.word_vectors = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Tokenize the text\n",
    "        tokenized_corpus = [nltk.word_tokenize(text.lower()) for text in tqdm(X, desc=\"Tokenizing for Word2Vec\")]\n",
    "        \n",
    "        # Train Word2Vec model\n",
    "        print(\"Training Word2Vec model...\")\n",
    "        self.model = Word2Vec(\n",
    "            sentences=tokenized_corpus,\n",
    "            vector_size=self.vector_size,\n",
    "            window=self.window,\n",
    "            min_count=self.min_count,\n",
    "            workers=self.workers,\n",
    "            sg=self.sg\n",
    "        )\n",
    "        \n",
    "        self.word_vectors = self.model.wv\n",
    "        print(f\"Word2Vec model trained with {len(self.word_vectors.key_to_index)} words\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        tokenized_corpus = [nltk.word_tokenize(text.lower()) for text in tqdm(X, desc=\"Vectorizing with Word2Vec\")]\n",
    "        \n",
    "        # Create document vectors by averaging word vectors\n",
    "        doc_vectors = np.zeros((len(tokenized_corpus), self.vector_size))\n",
    "        for i, tokens in enumerate(tokenized_corpus):\n",
    "            vec = np.zeros(self.vector_size)\n",
    "            count = 0\n",
    "            for token in tokens:\n",
    "                if token in self.word_vectors:\n",
    "                    vec += self.word_vectors[token]\n",
    "                    count += 1\n",
    "            if count > 0:\n",
    "                vec /= count\n",
    "            doc_vectors[i] = vec\n",
    "        \n",
    "        return doc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText Vectorizer\n",
    "class FastTextVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=100, window=5, min_count=1, workers=4):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.model = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Tokenize the text\n",
    "        tokenized_corpus = [nltk.word_tokenize(text.lower()) for text in tqdm(X, desc=\"Tokenizing for FastText\")]\n",
    "        \n",
    "        # Train FastText model\n",
    "        print(\"Training FastText model...\")\n",
    "        self.model = FastText(\n",
    "            sentences=tokenized_corpus,\n",
    "            vector_size=self.vector_size,\n",
    "            window=self.window,\n",
    "            min_count=self.min_count,\n",
    "            workers=self.workers\n",
    "        )\n",
    "        \n",
    "        print(f\"FastText model trained with {len(self.model.wv.key_to_index)} words\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        tokenized_corpus = [nltk.word_tokenize(text.lower()) for text in tqdm(X, desc=\"Vectorizing with FastText\")]\n",
    "        \n",
    "        # Create document vectors by averaging word vectors\n",
    "        doc_vectors = np.zeros((len(tokenized_corpus), self.vector_size))\n",
    "        for i, tokens in enumerate(tokenized_corpus):\n",
    "            vec = np.zeros(self.vector_size)\n",
    "            count = 0\n",
    "            for token in tokens:\n",
    "                # FastText can handle OOV words\n",
    "                vec += self.model.wv[token]\n",
    "                count += 1\n",
    "            if count > 0:\n",
    "                vec /= count\n",
    "            doc_vectors[i] = vec\n",
    "        \n",
    "        return doc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe - Using pre-trained embeddings\n",
    "class GloveVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=100):\n",
    "        self.vector_size = vector_size\n",
    "        self.word_vectors = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Attempt to download pre-trained GloVe using gensim downloader\n",
    "        try:\n",
    "            import gensim.downloader as api\n",
    "            print(\"Downloading pre-trained GloVe embeddings...\")\n",
    "            # Use a smaller model for demonstration\n",
    "            glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "            self.word_vectors = {word: glove_model[word] for word in glove_model.key_to_index}\n",
    "            print(f\"Loaded GloVe embeddings with {len(self.word_vectors)} words\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading GloVe: {e}\")\n",
    "            print(\"Will use an empty embedding. Results will be poor.\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        tokenized_corpus = [nltk.word_tokenize(text.lower()) for text in tqdm(X, desc=\"Vectorizing with GloVe\")]\n",
    "        \n",
    "        # Create document vectors by averaging word vectors\n",
    "        doc_vectors = np.zeros((len(tokenized_corpus), self.vector_size))\n",
    "        for i, tokens in enumerate(tokenized_corpus):\n",
    "            vec = np.zeros(self.vector_size)\n",
    "            count = 0\n",
    "            for token in tokens:\n",
    "                if token in self.word_vectors:\n",
    "                    vec += self.word_vectors[token]\n",
    "                    count += 1\n",
    "            if count > 0:\n",
    "                vec /= count\n",
    "            doc_vectors[i] = vec\n",
    "        \n",
    "        return doc_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Model Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, model_name):\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate accuracy and F1 score\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    \n",
    "    # Calculate F1 scores for each class\n",
    "    f1_scores = []\n",
    "    for i, column in enumerate(y.columns):\n",
    "        f1 = f1_score(y[column], y_pred[:, i])\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    # Calculate macro and micro F1\n",
    "    macro_f1 = np.mean(f1_scores)\n",
    "    micro_f1 = f1_score(y, y_pred, average='micro')\n",
    "    \n",
    "    print(f\"\\n============ {model_name} Results ============\")\n",
    "    print(f\"Inference time: {inference_time:.2f} seconds\")\n",
    "    print(f\"Validation accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "    print(f\"Micro F1: {micro_f1:.4f}\")\n",
    "    \n",
    "    print(\"\\nF1 scores by toxicity type:\")\n",
    "    for i, column in enumerate(y.columns):\n",
    "        f1 = f1_score(y[column], y_pred[:, i])\n",
    "        print(f\"{column}: {f1:.4f}\")\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'micro_f1': micro_f1,\n",
    "        'inference_time': inference_time\n",
    "    }\n",
    "    \n",
    "    for i, column in enumerate(y.columns):\n",
    "        results[f'f1_{column}'] = f1_scores[i]\n",
    "    \n",
    "    return results, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train and Evaluate Traditional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TF-IDF + SVM model...\n",
      "Training completed in 31.63 seconds\n",
      "\n",
      "============ TF-IDF + SVM Results ============\n",
      "Inference time: 1.15 seconds\n",
      "Validation accuracy: 0.8693\n",
      "Macro F1: 0.5317\n",
      "Micro F1: 0.6569\n",
      "\n",
      "F1 scores by toxicity type:\n",
      "toxic: 0.7220\n",
      "severe_toxic: 0.3953\n",
      "obscene: 0.7348\n",
      "threat: 0.3361\n",
      "insult: 0.6319\n",
      "identity_hate: 0.3702\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF + SVM\n",
    "tfidf_svm_model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=20000,\n",
    "        min_df=2,\n",
    "        max_df=0.8,\n",
    "        ngram_range=(1, 2)\n",
    "    )),\n",
    "    ('classifier', MultiOutputClassifier(LinearSVC(\n",
    "        C=1.0,\n",
    "        max_iter=10000,\n",
    "        dual=False,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )))\n",
    "])\n",
    "\n",
    "print(\"Training TF-IDF + SVM model...\")\n",
    "start_time = time.time()\n",
    "tfidf_svm_model.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "tfidf_svm_results, tfidf_svm_preds = evaluate_model(tfidf_svm_model, X_val, y_val, \"TF-IDF + SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Count Vectorizer + Logistic Regression model...\n",
      "Training completed in 79.33 seconds\n",
      "\n",
      "============ Count Vectorizer + Logistic Regression Results ============\n",
      "Inference time: 1.21 seconds\n",
      "Validation accuracy: 0.9120\n",
      "Macro F1: 0.3940\n",
      "Micro F1: 0.6100\n",
      "\n",
      "F1 scores by toxicity type:\n",
      "toxic: 0.7276\n",
      "severe_toxic: 0.2396\n",
      "obscene: 0.5845\n",
      "threat: 0.1138\n",
      "insult: 0.5394\n",
      "identity_hate: 0.1589\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer + Logistic Regression\n",
    "count_lr_model = Pipeline([\n",
    "    ('count', CountVectorizer(\n",
    "        max_features=20000,\n",
    "        min_df=2,\n",
    "        max_df=0.8,\n",
    "        ngram_range=(1, 2)\n",
    "    )),\n",
    "    ('classifier', MultiOutputClassifier(LogisticRegression(\n",
    "        C=5.0,\n",
    "        solver='liblinear',\n",
    "        max_iter=200,\n",
    "        random_state=42,\n",
    "        n_jobs=n_cores\n",
    "    )))\n",
    "])\n",
    "\n",
    "print(\"Training Count Vectorizer + Logistic Regression model...\")\n",
    "start_time = time.time()\n",
    "count_lr_model.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "count_lr_results, count_lr_preds = evaluate_model(count_lr_model, X_val, y_val, \"Count Vectorizer + Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Word2Vec vectorizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0a8fbe1b0d4637a9cda72bfe3130de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing for Word2Vec:   0%|          | 0/127656 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Word2Vec model trained with 177677 words\n",
      "Word2Vec training completed in 33.23 seconds\n",
      "Transforming training data with Word2Vec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6225a9b30994d3fa83cd087f164c59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vectorizing with Word2Vec:   0%|          | 0/127656 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec + SVM classifier...\n",
      "Classifier training completed in 10.71 seconds\n",
      "Transforming validation data with Word2Vec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e939f7cb064aa3afcf2f1aed77c55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vectorizing with Word2Vec:   0%|          | 0/31915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Word2Vec + SVM\n",
    "w2v_vectorizer = Word2VecVectorizer(vector_size=100, window=5, min_count=1, workers=n_cores, sg=1)\n",
    "\n",
    "# Fit the vectorizer to get word embeddings\n",
    "print(\"Fitting Word2Vec vectorizer...\")\n",
    "start_time = time.time()\n",
    "w2v_vectorizer.fit(X_train)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Word2Vec training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Transform training data\n",
    "print(\"Transforming training data with Word2Vec...\")\n",
    "X_train_w2v = w2v_vectorizer.transform(X_train)\n",
    "\n",
    "# # Train classifier\n",
    "# w2v_classifier = MultiOutputClassifier(LinearSVC(\n",
    "#     C=1.0,\n",
    "#     max_iter=10000,\n",
    "#     dual=False,\n",
    "#     class_weight='balanced',\n",
    "#     random_state=42\n",
    "# ))\n",
    "\n",
    "# print(\"Training Word2Vec + SVM classifier...\")\n",
    "# start_time = time.time()\n",
    "# w2v_classifier.fit(X_train_w2v, y_train)\n",
    "# train_time = time.time() - start_time\n",
    "# print(f\"Classifier training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Transform validation data and evaluate\n",
    "print(\"Transforming validation data with Word2Vec...\")\n",
    "X_val_w2v = w2v_vectorizer.transform(X_val)\n",
    "\n",
    "# # Create a class for evaluation that behaves like a pipeline\n",
    "# class ModelWrapper:\n",
    "#     def __init__(self, vectorizer, classifier):\n",
    "#         self.vectorizer = vectorizer\n",
    "#         self.classifier = classifier\n",
    "    \n",
    "#     def predict(self, X):\n",
    "#         X_transformed = self.vectorizer.transform(X)\n",
    "#         return self.classifier.predict(X_transformed)\n",
    "\n",
    "# w2v_model = ModelWrapper(w2v_vectorizer, w2v_classifier)\n",
    "# w2v_results, w2v_preds = evaluate_model(w2v_model, X_val, y_val, \"Word2Vec + SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for multiple classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from itertools import product\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train FastText Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting FastText vectorizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25b0a383d444f9fa9b929a1ca3cbd40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing for FastText:   0%|          | 0/127656 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FastText model...\n",
      "FastText model trained with 177677 words\n",
      "FastText training completed in 55.70 seconds\n",
      "Transforming training data with FastText...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be05a4a37e44d7aae8294b2e4b2cb08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vectorizing with FastText:   0%|          | 0/127656 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FastText + SVM classifier...\n",
      "Classifier training completed in 18.74 seconds\n",
      "Transforming validation data with FastText...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a908ab3f26a84eb6a28235e19aea880a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vectorizing with FastText:   0%|          | 0/31915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# FastText + SVM\n",
    "fasttext_vectorizer = FastTextVectorizer(vector_size=100, window=5, min_count=1, workers=n_cores)\n",
    "\n",
    "# Fit the vectorizer to get word embeddings\n",
    "print(\"Fitting FastText vectorizer...\")\n",
    "start_time = time.time()\n",
    "fasttext_vectorizer.fit(X_train)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"FastText training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Transform training data\n",
    "print(\"Transforming training data with FastText...\")\n",
    "X_train_fasttext = fasttext_vectorizer.transform(X_train)\n",
    "\n",
    "# # Train classifier\n",
    "# fasttext_classifier = MultiOutputClassifier(LinearSVC(\n",
    "#     C=1.0,\n",
    "#     max_iter=10000,\n",
    "#     dual=False,\n",
    "#     class_weight='balanced',\n",
    "#     random_state=42\n",
    "# ))\n",
    "\n",
    "# print(\"Training FastText + SVM classifier...\")\n",
    "# start_time = time.time()\n",
    "# fasttext_classifier.fit(X_train_fasttext, y_train)\n",
    "# train_time = time.time() - start_time\n",
    "# print(f\"Classifier training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Transform validation data and evaluate\n",
    "print(\"Transforming validation data with FastText...\")\n",
    "X_val_fasttext = fasttext_vectorizer.transform(X_val)\n",
    "\n",
    "# fasttext_model = ModelWrapper(fasttext_vectorizer, fasttext_classifier)\n",
    "# fasttext_results, fasttext_preds = evaluate_model(fasttext_model, X_val, y_val, \"FastText + SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train GloVe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting GloVe vectorizer...\n",
      "Downloading pre-trained GloVe embeddings...\n",
      "Loaded GloVe embeddings with 400000 words\n",
      "GloVe preparation completed in 17.00 seconds\n",
      "Transforming training data with GloVe...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75eab9f43eb42c2bac564bea19eec67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vectorizing with GloVe:   0%|          | 0/127656 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming validation data with GloVe...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6235718de04661bac49ad96d615871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vectorizing with GloVe:   0%|          | 0/31915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GloVe + SVM\n",
    "glove_vectorizer = GloveVectorizer(vector_size=100)\n",
    "\n",
    "# Fit the vectorizer to get word embeddings\n",
    "print(\"Fitting GloVe vectorizer...\")\n",
    "start_time = time.time()\n",
    "glove_vectorizer.fit(X_train)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"GloVe preparation completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Transform training data\n",
    "print(\"Transforming training data with GloVe...\")\n",
    "X_train_glove = glove_vectorizer.transform(X_train)\n",
    "\n",
    "# # Train classifier\n",
    "# glove_classifier = MultiOutputClassifier(LinearSVC(\n",
    "#     C=1.0,\n",
    "#     max_iter=10000,\n",
    "#     dual=False,\n",
    "#     class_weight='balanced',\n",
    "#     random_state=42\n",
    "# ))\n",
    "\n",
    "# print(\"Training GloVe + SVM classifier...\")\n",
    "# start_time = time.time()\n",
    "# glove_classifier.fit(X_train_glove, y_train)\n",
    "# train_time = time.time() - start_time\n",
    "# print(f\"Classifier training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Transform validation data and evaluate\n",
    "print(\"Transforming validation data with GloVe...\")\n",
    "X_val_glove = glove_vectorizer.transform(X_val)\n",
    "\n",
    "# glove_model = ModelWrapper(glove_vectorizer, glove_classifier)\n",
    "# glove_results, glove_preds = evaluate_model(glove_model, X_val, y_val, \"GloVe + SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_embeddings_with_multiple_classifiers(X_train_embedded, X_val_embedded, y_train, y_val, embedding_name):\n",
    "    \"\"\"\n",
    "    Evaluate multiple classifiers on the provided embedded features.\n",
    "    \n",
    "    Args:\n",
    "        X_train_embedded: Embedded training features\n",
    "        X_val_embedded: Embedded validation features\n",
    "        y_train: Training labels\n",
    "        y_val: Validation labels\n",
    "        embedding_name: Name of the embedding method\n",
    "        \n",
    "    Returns:\n",
    "        List of results for each classifier\n",
    "    \"\"\"\n",
    "    # Define classifiers to evaluate\n",
    "    classifiers = {\n",
    "        'SVM': MultiOutputClassifier(LinearSVC(\n",
    "            C=1.0, \n",
    "            max_iter=10000, \n",
    "            dual=False, \n",
    "            class_weight='balanced',\n",
    "            random_state=42\n",
    "        )),\n",
    "        \n",
    "        'LogisticRegression': MultiOutputClassifier(LogisticRegression(\n",
    "            C=1.0,\n",
    "            max_iter=1000,\n",
    "            solver='liblinear',\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=n_cores\n",
    "        )),\n",
    "        \n",
    "        'KNeighborsClassifier': MultiOutputClassifier(KNeighborsClassifier(\n",
    "            n_neighbors=5,\n",
    "            weights='distance',\n",
    "            n_jobs=n_cores\n",
    "        )),\n",
    "        \n",
    "        'XGBoost': MultiOutputClassifier(xgb.XGBClassifier(\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=100,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss',\n",
    "            random_state=42,\n",
    "            n_jobs=n_cores\n",
    "        ))\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Evaluate each classifier\n",
    "    for clf_name, classifier in classifiers.items():\n",
    "        model_name = f\"{embedding_name} + {clf_name}\"\n",
    "        print(f\"\\n===== Training {model_name} =====\")\n",
    "        \n",
    "        # Train classifier\n",
    "        start_time = time.time()\n",
    "        classifier.fit(X_train_embedded, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "        print(f\"Training completed in {train_time:.2f} seconds\")\n",
    "        \n",
    "        # Create a wrapper for evaluation\n",
    "        class SimpleModelWrapper:\n",
    "            def __init__(self, clf):\n",
    "                self.clf = clf\n",
    "            def predict(self, X):\n",
    "                return self.clf.predict(X)\n",
    "        \n",
    "        model = SimpleModelWrapper(classifier)\n",
    "        result, preds = evaluate_model(model, X_val_embedded, y_val, model_name)\n",
    "        result['train_time'] = train_time\n",
    "        results.append(result)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================ EVALUATING WORD2VEC EMBEDDINGS WITH MULTIPLE CLASSIFIERS ================\n",
      "\n",
      "===== Training Word2Vec + SVM =====\n",
      "Training completed in 10.84 seconds\n",
      "\n",
      "============ Word2Vec + SVM Results ============\n",
      "Inference time: 0.02 seconds\n",
      "Validation accuracy: 0.8071\n",
      "Macro F1: 0.3809\n",
      "Micro F1: 0.4703\n",
      "\n",
      "F1 scores by toxicity type:\n",
      "toxic: 0.6456\n",
      "severe_toxic: 0.2639\n",
      "obscene: 0.6135\n",
      "threat: 0.0686\n",
      "insult: 0.5319\n",
      "identity_hate: 0.1619\n",
      "\n",
      "===== Training Word2Vec + LogisticRegression =====\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store all results\n",
    "all_classifier_results = []\n",
    "\n",
    "# 1. Evaluate classifiers with Word2Vec embeddings\n",
    "print(\"\\n\\n================ EVALUATING WORD2VEC EMBEDDINGS WITH MULTIPLE CLASSIFIERS ================\")\n",
    "w2v_results = evaluate_embeddings_with_multiple_classifiers(\n",
    "    X_train_w2v, X_val_w2v, y_train, y_val, \"Word2Vec\"\n",
    ")\n",
    "all_classifier_results.extend(w2v_results)\n",
    "\n",
    "# 2. Evaluate classifiers with FastText embeddings\n",
    "print(\"\\n\\n================ EVALUATING FASTTEXT EMBEDDINGS WITH MULTIPLE CLASSIFIERS ================\")\n",
    "fasttext_results = evaluate_embeddings_with_multiple_classifiers(\n",
    "    X_train_fasttext, X_val_fasttext, y_train, y_val, \"FastText\"\n",
    ")\n",
    "all_classifier_results.extend(fasttext_results)\n",
    "\n",
    "# 3. Evaluate classifiers with GloVe embeddings\n",
    "print(\"\\n\\n================ EVALUATING GLOVE EMBEDDINGS WITH MULTIPLE CLASSIFIERS ================\")\n",
    "glove_results = evaluate_embeddings_with_multiple_classifiers(\n",
    "    X_train_glove, X_val_glove, y_train, y_val, \"GloVe\"\n",
    ")\n",
    "all_classifier_results.extend(glove_results)\n",
    "\n",
    "# Create a combined dataframe with all results\n",
    "combined_df = pd.DataFrame(all_classifier_results)\n",
    "\n",
    "# Display all results, sorted by macro F1 score\n",
    "print(\"\\n\\n================ COMBINED RESULTS FOR ALL EMBEDDINGS AND CLASSIFIERS ================\")\n",
    "print(combined_df[['model_name', 'accuracy', 'macro_f1', 'micro_f1', 'inference_time', 'train_time']].sort_values('macro_f1', ascending=False))\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(x='model_name', y='macro_f1', data=combined_df.sort_values('macro_f1', ascending=False).head(10))\n",
    "plt.title('Top 10 Models by Macro F1 Score')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Macro F1 Score')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize performance by toxicity category for top 5 models\n",
    "top_models = combined_df.sort_values('macro_f1', ascending=False).head(5)['model_name'].tolist()\n",
    "\n",
    "category_results = []\n",
    "for result in all_classifier_results:\n",
    "    if result['model_name'] in top_models:\n",
    "        model_name = result['model_name']\n",
    "        for col in y_val.columns:\n",
    "            category_results.append({\n",
    "                'model': model_name,\n",
    "                'category': col,\n",
    "                'f1_score': result[f'f1_{col}']\n",
    "            })\n",
    "\n",
    "category_df = pd.DataFrame(category_results)\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.barplot(x='category', y='f1_score', hue='model', data=category_df)\n",
    "plt.title('F1 Score by Toxicity Category for Top 5 Models')\n",
    "plt.xlabel('Toxicity Category')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Update the best model finding\n",
    "all_results.extend(all_classifier_results)\n",
    "results_df = pd.DataFrame(all_results)\n",
    "best_model_name = results_df.sort_values('macro_f1', ascending=False).iloc[0]['model_name']\n",
    "print(f\"\\nThe best performing model overall is: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "all_results = [tfidf_svm_results, count_lr_results, w2v_results, fasttext_results, glove_results]\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display comparison table\n",
    "print(\"\\n=========== Model Comparison ===========\\n\")\n",
    "comparison_cols = ['model_name', 'accuracy', 'macro_f1', 'micro_f1', 'inference_time']\n",
    "print(results_df[comparison_cols].sort_values('macro_f1', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize macro F1 scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='model_name', y='macro_f1', data=results_df.sort_values('macro_f1', ascending=False))\n",
    "plt.title('Macro F1 Score by Model')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Macro F1 Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize F1 scores by toxicity category\n",
    "category_results = []\n",
    "for result in all_results:\n",
    "    model_name = result['model_name']\n",
    "    for col in y_val.columns:\n",
    "        category_results.append({\n",
    "            'model': model_name,\n",
    "            'category': col,\n",
    "            'f1_score': result[f'f1_{col}']\n",
    "        })\n",
    "\n",
    "category_df = pd.DataFrame(category_results)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='category', y='f1_score', hue='model', data=category_df)\n",
    "plt.title('F1 Score by Toxicity Category and Model')\n",
    "plt.xlabel('Toxicity Category')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analysis and Findings\n",
    "\n",
    "Let's analyze the performance of different vectorization methods for toxic comment classification:\n",
    "\n",
    "1. **TF-IDF with SVM** generally performs well for text classification tasks, especially when we have a good vocabulary coverage.\n",
    "\n",
    "2. **Word2Vec** captures semantic relationships between words, which can help with understanding context beyond simple word presence.\n",
    "\n",
    "3. **FastText** can handle out-of-vocabulary words through subword information, which is particularly useful for toxic comments that often contain misspellings and made-up words.\n",
    "\n",
    "4. **GloVe** pre-trained embeddings capture global word co-occurrence statistics, which can provide good semantic representation.\n",
    "\n",
    "5. **Performance by category**: Note how different models perform on various toxicity categories. Some models might be better at detecting certain types of toxicity than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "Based on the results, we can make the following conclusions:\n",
    "\n",
    "1. For production use, the best model would be [determined by the best performer from the comparison above].\n",
    "\n",
    "2. Word embeddings can capture semantic meaning that might be missed by traditional bag-of-words approaches, potentially improving detection of subtle toxic content.\n",
    "\n",
    "3. FastText's ability to handle misspellings and rare words makes it particularly suitable for social media content and comments where users might intentionally obfuscate toxic language.\n",
    "\n",
    "4. Model selection should be based not just on overall performance but also on specific requirements, like performance on certain toxicity categories or inference time constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "best_model_name = results_df.sort_values('macro_f1', ascending=False).iloc[0]['model_name']\n",
    "print(f\"The best performing model is: {best_model_name}\")\n",
    "\n",
    "# Based on the model name, save the corresponding model\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "if best_model_name == \"TF-IDF + SVM\":\n",
    "    with open('../models/tfidf_svm_model.pkl', 'wb') as f:\n",
    "        pickle.dump(tfidf_svm_model, f)\n",
    "    print(\"Model saved as tfidf_svm_model.pkl\")\n",
    "elif best_model_name == \"Word2Vec + SVM\":\n",
    "    # For Word2Vec, save separately due to size\n",
    "    with open('../models/w2v_vectorizer.pkl', 'wb') as f:\n",
    "        pickle.dump(w2v_vectorizer, f)\n",
    "    with open('../models/w2v_classifier.pkl', 'wb') as f:\n",
    "        pickle.dump(w2v_classifier, f)\n",
    "    print(\"Model saved as w2v_vectorizer.pkl and w2v_classifier.pkl\")\n",
    "# Add similar conditions for other models\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
