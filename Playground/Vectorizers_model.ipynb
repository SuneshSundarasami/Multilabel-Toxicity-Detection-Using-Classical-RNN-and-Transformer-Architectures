{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Text Vectorization for Toxic Comment Classification\n",
    "\n",
    "This notebook compares traditional vectorization methods (TF-IDF, Count) with advanced embedding techniques (Word2Vec, GloVe, FastText) on the toxic comment classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import LinearSVC\n",
    "import nltk\n",
    "import time\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPU cores: 16\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "n_cores = multiprocessing.cpu_count()\n",
    "print(f\"Available CPU cores: {n_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (159571, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>daww match background colour im seemingly stuc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man im really not trying edit war guy cons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cant make real suggestion improvement wondered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sir hero chance remember page thats</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                      processed_text  \n",
       "0  explanation edits made username hardcore metal...  \n",
       "1  daww match background colour im seemingly stuc...  \n",
       "2  hey man im really not trying edit war guy cons...  \n",
       "3  cant make real suggestion improvement wondered...  \n",
       "4                sir hero chance remember page thats  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load preprocessed data\n",
    "train_data = pd.read_csv('../Dataset/train_preprocessed.csv')\n",
    "\n",
    "# Check the data\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 127656\n",
      "Validation set size: 31915\n"
     ]
    }
   ],
   "source": [
    "# Define the features and target labels\n",
    "X = train_data['processed_text']  # Use the preprocessed text\n",
    "y = train_data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "# Handle missing values\n",
    "X = X.fillna(\"\")  # Replace NaN values with empty strings\n",
    "y = y.fillna(0)   # Replace any missing target values with 0\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y['toxic']\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Advanced Vectorizers\n",
    "\n",
    "Let's implement custom vectorizers for Word2Vec, GloVe, and FastText embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Vectorizer\n",
    "class Word2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=100, window=5, min_count=1, workers=4, sg=1):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.sg = sg  # 0 for CBOW, 1 for Skip-gram\n",
    "        self.model = None\n",
    "        self.word_vectors = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Tokenize the text\n",
    "        tokenized_corpus = [nltk.word_tokenize(text.lower()) for text in tqdm(X, desc=\"Tokenizing for Word2Vec\")]\n",
    "        \n",
    "        # Train Word2Vec model\n",
    "        print(\"Training Word2Vec model...\")\n",
    "        self.model = Word2Vec(\n",
    "            sentences=tokenized_corpus,\n",
    "            vector_size=self.vector_size,\n",
    "            window=self.window,\n",
    "            min_count=self.min_count,\n",
    "            workers=self.workers,\n",
    "            sg=self.sg\n",
    "        )\n",
    "        \n",
    "        self.word_vectors = self.model.wv\n",
    "        print(f\"Word2Vec model trained with {len(self.word_vectors.key_to_index)} words\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        tokenized_corpus = [nltk.word_tokenize(text.lower()) for text in tqdm(X, desc=\"Vectorizing with Word2Vec\")]\n",
    "        \n",
    "        # Create document vectors by averaging word vectors\n",
    "        doc_vectors = np.zeros((len(tokenized_corpus), self.vector_size))\n",
    "        for i, tokens in enumerate(tokenized_corpus):\n",
    "            vec = np.zeros(self.vector_size)\n",
    "            count = 0\n",
    "            for token in tokens:\n",
    "                if token in self.word_vectors:\n",
    "                    vec += self.word_vectors[token]\n",
    "                    count += 1\n",
    "            if count > 0:\n",
    "                vec /= count\n",
    "            doc_vectors[i] = vec\n",
    "        \n",
    "        return doc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText Vectorizer\n",
    "class FastTextVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=100, window=5, min_count=1, workers=4):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.model = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Tokenize the text\n",
    "        tokenized_corpus = [nltk.word_tokenize(text.lower()) for text in tqdm(X, desc=\"Tokenizing for FastText\")]\n",
    "        \n",
    "        # Train FastText model\n",
    "        print(\"Training FastText model...\")\n",
    "        self.model = FastText(\n",
    "            sentences=tokenized_corpus,\n",
    "            vector_size=self.vector_size,\n",
    "            window=self.window,\n",
    "            min_count=self.min_count,\n",
    "            workers=self.workers\n",
    "        )\n",
    "        \n",
    "        print(f\"FastText model trained with {len(self.model.wv.key_to_index)} words\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        tokenized_corpus = [nltk.word_tokenize(text.lower()) for text in tqdm(X, desc=\"Vectorizing with FastText\")]\n",
    "        \n",
    "        # Create document vectors by averaging word vectors\n",
    "        doc_vectors = np.zeros((len(tokenized_corpus), self.vector_size))\n",
    "        for i, tokens in enumerate(tokenized_corpus):\n",
    "            vec = np.zeros(self.vector_size)\n",
    "            count = 0\n",
    "            for token in tokens:\n",
    "                # FastText can handle OOV words\n",
    "                vec += self.model.wv[token]\n",
    "                count += 1\n",
    "            if count > 0:\n",
    "                vec /= count\n",
    "            doc_vectors[i] = vec\n",
    "        \n",
    "        return doc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe - Using pre-trained embeddings\n",
    "class GloveVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=100):\n",
    "        self.vector_size = vector_size\n",
    "        self.word_vectors = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Attempt to download pre-trained GloVe using gensim downloader\n",
    "        try:\n",
    "            import gensim.downloader as api\n",
    "            print(\"Downloading pre-trained GloVe embeddings...\")\n",
    "            # Use a smaller model for demonstration\n",
    "            glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "            self.word_vectors = {word: glove_model[word] for word in glove_model.key_to_index}\n",
    "            print(f\"Loaded GloVe embeddings with {len(self.word_vectors)} words\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading GloVe: {e}\")\n",
    "            print(\"Will use an empty embedding. Results will be poor.\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        tokenized_corpus = [nltk.word_tokenize(text.lower()) for text in tqdm(X, desc=\"Vectorizing with GloVe\")]\n",
    "        \n",
    "        # Create document vectors by averaging word vectors\n",
    "        doc_vectors = np.zeros((len(tokenized_corpus), self.vector_size))\n",
    "        for i, tokens in enumerate(tokenized_corpus):\n",
    "            vec = np.zeros(self.vector_size)\n",
    "            count = 0\n",
    "            for token in tokens:\n",
    "                if token in self.word_vectors:\n",
    "                    vec += self.word_vectors[token]\n",
    "                    count += 1\n",
    "            if count > 0:\n",
    "                vec /= count\n",
    "            doc_vectors[i] = vec\n",
    "        \n",
    "        return doc_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Model Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, model_name):\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate accuracy and F1 score\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    \n",
    "    # Calculate F1 scores for each class\n",
    "    f1_scores = []\n",
    "    for i, column in enumerate(y.columns):\n",
    "        f1 = f1_score(y[column], y_pred[:, i])\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    # Calculate macro and micro F1\n",
    "    macro_f1 = np.mean(f1_scores)\n",
    "    micro_f1 = f1_score(y, y_pred, average='micro')\n",
    "    \n",
    "    print(f\"\\n============ {model_name} Results ============\")\n",
    "    print(f\"Inference time: {inference_time:.2f} seconds\")\n",
    "    print(f\"Validation accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "    print(f\"Micro F1: {micro_f1:.4f}\")\n",
    "    \n",
    "    print(\"\\nF1 scores by toxicity type:\")\n",
    "    for i, column in enumerate(y.columns):\n",
    "        f1 = f1_score(y[column], y_pred[:, i])\n",
    "        print(f\"{column}: {f1:.4f}\")\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'micro_f1': micro_f1,\n",
    "        'inference_time': inference_time\n",
    "    }\n",
    "    \n",
    "    for i, column in enumerate(y.columns):\n",
    "        results[f'f1_{column}'] = f1_scores[i]\n",
    "    \n",
    "    return results, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train and Evaluate Traditional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TF-IDF + SVM model...\n",
      "Training completed in 30.53 seconds\n",
      "\n",
      "============ TF-IDF + SVM Results ============\n",
      "Inference time: 1.11 seconds\n",
      "Validation accuracy: 0.8693\n",
      "Macro F1: 0.5317\n",
      "Micro F1: 0.6569\n",
      "\n",
      "F1 scores by toxicity type:\n",
      "toxic: 0.7220\n",
      "severe_toxic: 0.3953\n",
      "obscene: 0.7348\n",
      "threat: 0.3361\n",
      "insult: 0.6319\n",
      "identity_hate: 0.3702\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF + SVM\n",
    "tfidf_svm_model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=20000,\n",
    "        min_df=2,\n",
    "        max_df=0.8,\n",
    "        ngram_range=(1, 2)\n",
    "    )),\n",
    "    ('classifier', MultiOutputClassifier(LinearSVC(\n",
    "        C=1.0,\n",
    "        max_iter=10000,\n",
    "        dual=False,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )))\n",
    "])\n",
    "\n",
    "print(\"Training TF-IDF + SVM model...\")\n",
    "start_time = time.time()\n",
    "tfidf_svm_model.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "tfidf_svm_results, tfidf_svm_preds = evaluate_model(tfidf_svm_model, X_val, y_val, \"TF-IDF + SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Count Vectorizer + Logistic Regression model...\n",
      "Training completed in 82.05 seconds\n",
      "\n",
      "============ Count Vectorizer + Logistic Regression Results ============\n",
      "Inference time: 1.10 seconds\n",
      "Validation accuracy: 0.9120\n",
      "Macro F1: 0.3940\n",
      "Micro F1: 0.6100\n",
      "\n",
      "F1 scores by toxicity type:\n",
      "toxic: 0.7276\n",
      "severe_toxic: 0.2396\n",
      "obscene: 0.5845\n",
      "threat: 0.1138\n",
      "insult: 0.5394\n",
      "identity_hate: 0.1589\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer + Logistic Regression\n",
    "count_lr_model = Pipeline([\n",
    "    ('count', CountVectorizer(\n",
    "        max_features=20000,\n",
    "        min_df=2,\n",
    "        max_df=0.8,\n",
    "        ngram_range=(1, 2)\n",
    "    )),\n",
    "    ('classifier', MultiOutputClassifier(LogisticRegression(\n",
    "        C=5.0,\n",
    "        solver='liblinear',\n",
    "        max_iter=200,\n",
    "        random_state=42,\n",
    "        n_jobs=n_cores\n",
    "    )))\n",
    "])\n",
    "\n",
    "print(\"Training Count Vectorizer + Logistic Regression model...\")\n",
    "start_time = time.time()\n",
    "count_lr_model.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "count_lr_results, count_lr_preds = evaluate_model(count_lr_model, X_val, y_val, \"Count Vectorizer + Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and Evaluate Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Word2Vec vectorizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dadb459ef9044aaf9231871c34d41d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing for Word2Vec:   0%|          | 0/127656 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Word2Vec model trained with 177677 words\n",
      "Word2Vec training completed in 32.67 seconds\n",
      "Transforming training data with Word2Vec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000cab755dc045e4afd528f3000dad5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vectorizing with Word2Vec:   0%|          | 0/127656 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "LinearSVC.__init__() got an unexpected keyword argument 'n_jobs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m X_train_w2v \u001b[38;5;241m=\u001b[39m w2v_vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_train)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train classifier\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m w2v_classifier \u001b[38;5;241m=\u001b[39m MultiOutputClassifier(\u001b[43mLinearSVC\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdual\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbalanced\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_cores\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Word2Vec + SVM classifier...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mTypeError\u001b[0m: LinearSVC.__init__() got an unexpected keyword argument 'n_jobs'"
     ]
    }
   ],
   "source": [
    "# Word2Vec + SVM\n",
    "w2v_vectorizer = Word2VecVectorizer(vector_size=100, window=5, min_count=1, workers=n_cores, sg=1)\n",
    "\n",
    "# Fit the vectorizer to get word embeddings\n",
    "print(\"Fitting Word2Vec vectorizer...\")\n",
    "start_time = time.time()\n",
    "w2v_vectorizer.fit(X_train)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Word2Vec training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Transform training data\n",
    "print(\"Transforming training data with Word2Vec...\")\n",
    "X_train_w2v = w2v_vectorizer.transform(X_train)\n",
    "\n",
    "# Train classifier\n",
    "w2v_classifier = MultiOutputClassifier(LinearSVC(\n",
    "    C=1.0,\n",
    "    max_iter=10000,\n",
    "    dual=False,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    "))\n",
    "\n",
    "print(\"Training Word2Vec + SVM classifier...\")\n",
    "start_time = time.time()\n",
    "w2v_classifier.fit(X_train_w2v, y_train)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Classifier training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Transform validation data and evaluate\n",
    "print(\"Transforming validation data with Word2Vec...\")\n",
    "X_val_w2v = w2v_vectorizer.transform(X_val)\n",
    "\n",
    "# Create a class for evaluation that behaves like a pipeline\n",
    "class ModelWrapper:\n",
    "    def __init__(self, vectorizer, classifier):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.classifier = classifier\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_transformed = self.vectorizer.transform(X)\n",
    "        return self.classifier.predict(X_transformed)\n",
    "\n",
    "w2v_model = ModelWrapper(w2v_vectorizer, w2v_classifier)\n",
    "w2v_results, w2v_preds = evaluate_model(w2v_model, X_val, y_val, \"Word2Vec + SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and Evaluate FastText Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting FastText vectorizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c2e292c2734ddd8e6c84a2d49d8a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing for FastText:   0%|          | 0/127656 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FastText model...\n",
      "FastText model trained with 177677 words\n",
      "FastText training completed in 70.58 seconds\n",
      "Transforming training data with FastText...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062aae67b80643f48f31097e0c88de74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vectorizing with FastText:   0%|          | 0/127656 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FastText + SVM classifier...\n",
      "Classifier training completed in 25.92 seconds\n",
      "Transforming validation data with FastText...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0052c7799f84178b87a735c60c494ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vectorizing with FastText:   0%|          | 0/31915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010bf6a85c1643f59d9978134981f32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vectorizing with FastText:   0%|          | 0/31915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ FastText + SVM Results ============\n",
      "Inference time: 9.50 seconds\n",
      "Validation accuracy: 0.7869\n",
      "Macro F1: 0.3479\n",
      "Micro F1: 0.4254\n",
      "\n",
      "F1 scores by toxicity type:\n",
      "toxic: 0.5972\n",
      "severe_toxic: 0.2493\n",
      "obscene: 0.5490\n",
      "threat: 0.0567\n",
      "insult: 0.4870\n",
      "identity_hate: 0.1483\n"
     ]
    }
   ],
   "source": [
    "# FastText + SVM\n",
    "fasttext_vectorizer = FastTextVectorizer(vector_size=100, window=5, min_count=1, workers=n_cores)\n",
    "\n",
    "# Fit the vectorizer to get word embeddings\n",
    "print(\"Fitting FastText vectorizer...\")\n",
    "start_time = time.time()\n",
    "fasttext_vectorizer.fit(X_train)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"FastText training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Transform training data\n",
    "print(\"Transforming training data with FastText...\")\n",
    "X_train_fasttext = fasttext_vectorizer.transform(X_train)\n",
    "\n",
    "# Train classifier\n",
    "fasttext_classifier = MultiOutputClassifier(LinearSVC(\n",
    "    C=1.0,\n",
    "    max_iter=10000,\n",
    "    dual=False,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=n_cores\n",
    "))\n",
    "\n",
    "print(\"Training FastText + SVM classifier...\")\n",
    "start_time = time.time()\n",
    "fasttext_classifier.fit(X_train_fasttext, y_train)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Classifier training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Transform validation data and evaluate\n",
    "print(\"Transforming validation data with FastText...\")\n",
    "X_val_fasttext = fasttext_vectorizer.transform(X_val)\n",
    "\n",
    "fasttext_model = ModelWrapper(fasttext_vectorizer, fasttext_classifier)\n",
    "fasttext_results, fasttext_preds = evaluate_model(fasttext_model, X_val, y_val, \"FastText + SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train and Evaluate GloVe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting GloVe vectorizer...\n",
      "Downloading pre-trained GloVe embeddings...\n"
     ]
    }
   ],
   "source": [
    "# GloVe + SVM\n",
    "glove_vectorizer = GloveVectorizer(vector_size=100)\n",
    "\n",
    "# Fit the vectorizer to get word embeddings\n",
    "print(\"Fitting GloVe vectorizer...\")\n",
    "start_time = time.time()\n",
    "glove_vectorizer.fit(X_train)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"GloVe preparation completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Transform training data\n",
    "print(\"Transforming training data with GloVe...\")\n",
    "X_train_glove = glove_vectorizer.transform(X_train)\n",
    "\n",
    "# Train classifier\n",
    "glove_classifier = MultiOutputClassifier(LinearSVC(\n",
    "    C=1.0,\n",
    "    max_iter=10000,\n",
    "    dual=False,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=n_cores\n",
    "))\n",
    "\n",
    "print(\"Training GloVe + SVM classifier...\")\n",
    "start_time = time.time()\n",
    "glove_classifier.fit(X_train_glove, y_train)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"Classifier training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Transform validation data and evaluate\n",
    "print(\"Transforming validation data with GloVe...\")\n",
    "X_val_glove = glove_vectorizer.transform(X_val)\n",
    "\n",
    "glove_model = ModelWrapper(glove_vectorizer, glove_classifier)\n",
    "glove_results, glove_preds = evaluate_model(glove_model, X_val, y_val, \"GloVe + SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "all_results = [tfidf_svm_results, count_lr_results, w2v_results, fasttext_results, glove_results]\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display comparison table\n",
    "print(\"\\n=========== Model Comparison ===========\\n\")\n",
    "comparison_cols = ['model_name', 'accuracy', 'macro_f1', 'micro_f1', 'inference_time']\n",
    "print(results_df[comparison_cols].sort_values('macro_f1', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize macro F1 scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='model_name', y='macro_f1', data=results_df.sort_values('macro_f1', ascending=False))\n",
    "plt.title('Macro F1 Score by Model')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Macro F1 Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize F1 scores by toxicity category\n",
    "category_results = []\n",
    "for result in all_results:\n",
    "    model_name = result['model_name']\n",
    "    for col in y_val.columns:\n",
    "        category_results.append({\n",
    "            'model': model_name,\n",
    "            'category': col,\n",
    "            'f1_score': result[f'f1_{col}']\n",
    "        })\n",
    "\n",
    "category_df = pd.DataFrame(category_results)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='category', y='f1_score', hue='model', data=category_df)\n",
    "plt.title('F1 Score by Toxicity Category and Model')\n",
    "plt.xlabel('Toxicity Category')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analysis and Findings\n",
    "\n",
    "Let's analyze the performance of different vectorization methods for toxic comment classification:\n",
    "\n",
    "1. **TF-IDF with SVM** generally performs well for text classification tasks, especially when we have a good vocabulary coverage.\n",
    "\n",
    "2. **Word2Vec** captures semantic relationships between words, which can help with understanding context beyond simple word presence.\n",
    "\n",
    "3. **FastText** can handle out-of-vocabulary words through subword information, which is particularly useful for toxic comments that often contain misspellings and made-up words.\n",
    "\n",
    "4. **GloVe** pre-trained embeddings capture global word co-occurrence statistics, which can provide good semantic representation.\n",
    "\n",
    "5. **Performance by category**: Note how different models perform on various toxicity categories. Some models might be better at detecting certain types of toxicity than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "Based on the results, we can make the following conclusions:\n",
    "\n",
    "1. For production use, the best model would be [determined by the best performer from the comparison above].\n",
    "\n",
    "2. Word embeddings can capture semantic meaning that might be missed by traditional bag-of-words approaches, potentially improving detection of subtle toxic content.\n",
    "\n",
    "3. FastText's ability to handle misspellings and rare words makes it particularly suitable for social media content and comments where users might intentionally obfuscate toxic language.\n",
    "\n",
    "4. Model selection should be based not just on overall performance but also on specific requirements, like performance on certain toxicity categories or inference time constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "best_model_name = results_df.sort_values('macro_f1', ascending=False).iloc[0]['model_name']\n",
    "print(f\"The best performing model is: {best_model_name}\")\n",
    "\n",
    "# Based on the model name, save the corresponding model\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "if best_model_name == \"TF-IDF + SVM\":\n",
    "    with open('../models/tfidf_svm_model.pkl', 'wb') as f:\n",
    "        pickle.dump(tfidf_svm_model, f)\n",
    "    print(\"Model saved as tfidf_svm_model.pkl\")\n",
    "elif best_model_name == \"Word2Vec + SVM\":\n",
    "    # For Word2Vec, save separately due to size\n",
    "    with open('../models/w2v_vectorizer.pkl', 'wb') as f:\n",
    "        pickle.dump(w2v_vectorizer, f)\n",
    "    with open('../models/w2v_classifier.pkl', 'wb') as f:\n",
    "        pickle.dump(w2v_classifier, f)\n",
    "    print(\"Model saved as w2v_vectorizer.pkl and w2v_classifier.pkl\")\n",
    "# Add similar conditions for other models\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
