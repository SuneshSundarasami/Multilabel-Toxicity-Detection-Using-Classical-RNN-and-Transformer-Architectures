{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49141409",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U \"numpy<2.0\" \"matplotlib==3.8.2\" scikit-learn pandas \"transformers==4.44.2\" seaborn tqdm \"torch==2.4.0\" --force-reinstall --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1476ba5",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Force single GPU usage\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2450f8ab",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "\n",
    "# Model configuration for inference\n",
    "MAX_LENGTH = 512  # Maximum input length for T5\n",
    "GENERATION_MAX_LENGTH = 10  \n",
    "BATCH_SIZE = 8  # Batch size for inference\n",
    "\n",
    "# Adjust this path to point to your local dataset location\n",
    "TRAIN_DATA_PATH = \"/kaggle/input/jigsaw-toxic-comment-classification-preprocessed/train_preprocessed.csv\" \n",
    "OUTPUT_DIR = \"/kaggle/working/\"\n",
    "TEXT_COLUMN = \"comment_text\" \n",
    "LABEL_COLUMNS = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "NUM_LABELS = len(LABEL_COLUMNS)\n",
    "\n",
    "# Defining comprehensive few-shot prompts and descriptions for each toxicity category\n",
    "PROMPT_TEMPLATES = {\n",
    "    \"toxic\": \"\"\"A toxic comment is a rude, disrespectful, or unreasonable utterance that is likely to make people leave a discussion.\n",
    "\n",
    "Examples of toxic comments:\n",
    "1. \"Your opinion is completely off-base and pointless—you don't bring anything of value to this conversation.\"\n",
    "2. \"Honestly, I can't believe you're still typing—this is just mind-numbing fluff.\"\n",
    "3. \"This entire thread is embarrassing; you clearly don't know what you're talking about.\"\n",
    "4. \"Can you please stop wasting everyone's time with this garbage?\"\n",
    "5. \"Reading your posts is exhausting; maybe you should rethink posting at all.\"\n",
    "\n",
    "Based on the definition and examples above, is this comment toxic? Answer yes or no.\n",
    "Comment: {text}\"\"\",\n",
    "\n",
    "    \"severe_toxic\": \"\"\"A severe_toxic comment exhibits very hateful or aggressive language—far beyond mere profanity—such that it is highly likely to drive participants away or cause them to abandon sharing their perspective.\n",
    "\n",
    "Examples of severely toxic comments:\n",
    "1. \"You are an absolute disgrace—no one wants to hear your toxic drivel; just disappear.\"\n",
    "2. \"I sincerely hope you get banned and never show your pathetic face here again.\"\n",
    "3. \"Your comments are the vile rubbish of the internet, and I want you gone.\"\n",
    "4. \"Nobody cares about your warped opinions, and you'd be doing us all a favor shutting up forever.\"\n",
    "5. \"This is filth, pure filth—and it's poisoning everyone's time; stop posting or get banned.\"\n",
    "\n",
    "Based on the definition and examples above, is this comment severely toxic? Answer yes or no.\n",
    "Comment: {text}\"\"\",\n",
    "\n",
    "    \"obscene\": \"\"\"An obscene comment contains vulgar or profane language (e.g., explicit curse words or sexual references) intended to shock or offend.\n",
    "\n",
    "Examples of obscene comments:\n",
    "1. \"You're a fucking clown, and your posts are full of utter bullshit.\"\n",
    "2. \"Go fuck yourself—you don't have a clue what you're talking about.\"\n",
    "3. \"That was some pathetic horny rambling—get out of here with that smut.\"\n",
    "4. \"Your comment is pure disgusting crap—full of stupid fucking lies.\"\n",
    "5. \"You piece of shit, stop spewing your obscene drivel at us.\"\n",
    "\n",
    "Based on the definition and examples above, is this comment obscene? Answer yes or no.\n",
    "Comment: {text}\"\"\",\n",
    "\n",
    "    \"threat\": \"\"\"A threat is a statement expressing an intention to inflict pain, injury, damage, or other hostile action against an individual or group, or to coerce them by implying future harm.\n",
    "\n",
    "Examples of threatening comments:\n",
    "1. \"If you don't shut up, I'm going to make sure you regret it.\"\n",
    "2. \"Try posting that again and I'll find out where you live.\"\n",
    "3. \"You better delete that now or you'll pay the price.\"\n",
    "4. \"Keep talking like that and I'll come after you personally.\"\n",
    "5. \"One more smart comment from you and I guarantee you'll be sorry.\"\n",
    "\n",
    "Based on the definition and examples above, does this comment contain a threat? Answer yes or no.\n",
    "Comment: {text}\"\"\",\n",
    "\n",
    "    \"insult\": \"\"\"An insult is an abusive, inflammatory, or negative remark directed at a person or group, intended to demean, belittle, or provoke.\n",
    "\n",
    "Examples of insulting comments:\n",
    "1. \"You're the most clueless person I've seen online—embarrassing.\"\n",
    "2. \"Nice rant, genius—you'd fail a kindergarten test.\"\n",
    "3. \"That idea was so stupid I can't believe you even wrote it.\"\n",
    "4. \"You're pathetic—someone as ignorant as you shouldn't be posting in public.\"\n",
    "5. \"Thanks for your insight, Captain Obvious—you're really helpful (not).\"\n",
    "\n",
    "Based on the definition and examples above, is this comment insulting? Answer yes or no.\n",
    "Comment: {text}\"\"\",\n",
    "\n",
    "    \"identity_hate\": \"\"\"An identity_hate comment targets a person or group based on a protected characteristic (e.g., race, religion, gender identity, sexual orientation), conveying negative or hateful sentiments toward that identity.\n",
    "\n",
    "Examples of identity-based hate comments:\n",
    "1. \"Of course you'd say that—that's typical of your religion, always acting like that.\"\n",
    "2. \"People from your country are worthless and should be ashamed.\"\n",
    "3. \"Why are gays always so annoying? Just go away.\"\n",
    "4. \"You're only smart for a woman—too bad you're just a girl.\"\n",
    "5. \"I can't stand black people—they ruin everything they touch.\"\n",
    "\n",
    "Based on the definition and examples above, does this comment contain identity-based hate? Answer yes or no.\n",
    "Comment: {text}\"\"\"\n",
    "}\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Label columns: {LABEL_COLUMNS}\")\n",
    "print(f\"Number of labels: {NUM_LABELS}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2011f7f5-a5a7-4035-bc70-02362f6019e4",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loading and Splitting Dataset into Train/Validation/Test\n",
    "print(\"Loading preprocessed training data...\")\n",
    "try:\n",
    "    full_train_df = pd.read_csv(TRAIN_DATA_PATH)\n",
    "    print(f\" Training data loaded successfully: {full_train_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\" Training data not found at: {TRAIN_DATA_PATH}\")\n",
    "    print(\"Please update paths in the configuration section\")\n",
    "    raise\n",
    "\n",
    "# Displaying basic information about the data\n",
    "print(\"\\nOriginal training data columns:\")\n",
    "print(full_train_df.columns.tolist())\n",
    "\n",
    "# Check if required columns exist\n",
    "required_columns = [TEXT_COLUMN] + LABEL_COLUMNS\n",
    "missing_columns = [col for col in required_columns if col not in full_train_df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"\\n Missing required columns in training data: {missing_columns}\")\n",
    "    print(f\"Available columns: {full_train_df.columns.tolist()}\")\n",
    "    print(\"\\nPlease ensure your preprocessed data has the following columns:\")\n",
    "    print(f\"- {TEXT_COLUMN} (the processed text)\")\n",
    "    print(f\"- {', '.join(LABEL_COLUMNS)} (label columns)\")\n",
    "    raise ValueError(\"Missing required columns\")\n",
    "else:\n",
    "    print(\"\\n All required columns found in training data\")\n",
    "\n",
    "print(\"\\nFirst few rows of original training data:\")\n",
    "print(full_train_df.head())\n",
    "\n",
    "# Check label distribution in original data\n",
    "print(\"\\nLabel distribution in original training data:\")\n",
    "label_stats = full_train_df[LABEL_COLUMNS].sum()\n",
    "print(label_stats)\n",
    "\n",
    "# Calculate percentage of positive labels\n",
    "print(\"\\nPercentage of positive labels:\")\n",
    "label_percentages = (full_train_df[LABEL_COLUMNS].sum() / len(full_train_df)) * 100\n",
    "print(label_percentages)\n",
    "\n",
    "# Visualize original label distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "label_stats.plot(kind='bar')\n",
    "plt.title('Label Counts in Original Training Data')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "label_percentages.plot(kind='bar')\n",
    "plt.title('Label Percentages in Original Training Data')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Split the data into train/validation/test (70%/15%/15%)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SPLITTING DATA INTO TRAIN/VALIDATION/TEST\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# First split: separate test set (15% of total data)\n",
    "train_val_df, test_df = train_test_split(\n",
    "    full_train_df, \n",
    "    test_size=0.15, \n",
    "    random_state=42,\n",
    "    stratify=full_train_df[LABEL_COLUMNS[0]]  # Stratify on primary toxic label\n",
    ")\n",
    "\n",
    "# Second split: separate validation from remaining data (15% of total = ~17.6% of remaining)\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df,\n",
    "    test_size=0.176,  # This gives us ~15% of original data for validation\n",
    "    random_state=42,\n",
    "    stratify=train_val_df[LABEL_COLUMNS[0]]  # Stratify on primary toxic label\n",
    ")\n",
    "\n",
    "print(f\"Data split completed:\")\n",
    "print(f\"- Training set: {len(train_df):,} samples ({len(train_df)/len(full_train_df)*100:.1f}%)\")\n",
    "print(f\"- Validation set: {len(val_df):,} samples ({len(val_df)/len(full_train_df)*100:.1f}%)\")\n",
    "print(f\"- Test set: {len(test_df):,} samples ({len(test_df)/len(full_train_df)*100:.1f}%)\")\n",
    "print(f\"- Total: {len(train_df) + len(val_df) + len(test_df):,} samples\")\n",
    "\n",
    "# Create test set without labels for prediction\n",
    "test_df_no_labels = test_df[[TEXT_COLUMN]].copy()\n",
    "if 'id' not in test_df.columns:\n",
    "    # Create an ID column if it doesn't exist\n",
    "    test_df_no_labels['id'] = range(len(test_df_no_labels))\n",
    "    test_df['id'] = range(len(test_df))\n",
    "else:\n",
    "    test_df_no_labels['id'] = test_df['id'].copy()\n",
    "\n",
    "# Save the split datasets\n",
    "train_split_path = f\"{OUTPUT_DIR}/train_split.csv\"\n",
    "val_split_path = f\"{OUTPUT_DIR}/val_split.csv\"\n",
    "test_split_with_labels_path = f\"{OUTPUT_DIR}/test_split_with_labels.csv\"\n",
    "test_split_no_labels_path = f\"{OUTPUT_DIR}/test_split_no_labels.csv\"\n",
    "\n",
    "train_df.to_csv(train_split_path, index=False)\n",
    "val_df.to_csv(val_split_path, index=False)\n",
    "test_df.to_csv(test_split_with_labels_path, index=False)\n",
    "test_df_no_labels.to_csv(test_split_no_labels_path, index=False)\n",
    "\n",
    "print(f\"\\n Split datasets saved:\")\n",
    "print(f\"- Training data: {train_split_path}\")\n",
    "print(f\"- Validation data: {val_split_path}\")\n",
    "print(f\"- Test data (with labels): {test_split_with_labels_path}\")\n",
    "print(f\"- Test data (without labels): {test_split_no_labels_path}\")\n",
    "\n",
    "# Compare label distributions across splits\n",
    "print(f\"\\n LABEL DISTRIBUTION COMPARISON:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "splits_info = {\n",
    "    'Original': full_train_df,\n",
    "    'Train': train_df,\n",
    "    'Validation': val_df,\n",
    "    'Test': test_df\n",
    "}\n",
    "\n",
    "comparison_data = []\n",
    "for split_name, split_df in splits_info.items():\n",
    "    row = {'Split': split_name, 'Size': len(split_df)}\n",
    "    for label in LABEL_COLUMNS:\n",
    "        count = split_df[label].sum()\n",
    "        percentage = (count / len(split_df)) * 100\n",
    "        row[f'{label}_count'] = count\n",
    "        row[f'{label}_pct'] = percentage\n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nLabel counts and percentages by split:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize distribution comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Label Distribution Comparison Across Splits', fontsize=16)\n",
    "\n",
    "# Plot 1: Sample counts\n",
    "splits = ['Train', 'Validation', 'Test']\n",
    "sizes = [len(train_df), len(val_df), len(test_df)]\n",
    "axes[0, 0].bar(splits, sizes, color=['skyblue', 'lightgreen', 'lightcoral'])\n",
    "axes[0, 0].set_title('Sample Counts by Split')\n",
    "axes[0, 0].set_ylabel('Number of Samples')\n",
    "for i, v in enumerate(sizes):\n",
    "    axes[0, 0].text(i, v + max(sizes)*0.01, f'{v:,}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 2: Label percentages for each split\n",
    "label_pcts = {\n",
    "    'Train': [(train_df[label].sum() / len(train_df)) * 100 for label in LABEL_COLUMNS],\n",
    "    'Validation': [(val_df[label].sum() / len(val_df)) * 100 for label in LABEL_COLUMNS],\n",
    "    'Test': [(test_df[label].sum() / len(test_df)) * 100 for label in LABEL_COLUMNS]\n",
    "}\n",
    "\n",
    "x = np.arange(len(LABEL_COLUMNS))\n",
    "width = 0.25\n",
    "\n",
    "axes[0, 1].bar(x - width, label_pcts['Train'], width, label='Train', color='skyblue')\n",
    "axes[0, 1].bar(x, label_pcts['Validation'], width, label='Validation', color='lightgreen')\n",
    "axes[0, 1].bar(x + width, label_pcts['Test'], width, label='Test', color='lightcoral')\n",
    "\n",
    "axes[0, 1].set_title('Label Percentages by Split')\n",
    "axes[0, 1].set_xlabel('Labels')\n",
    "axes[0, 1].set_ylabel('Percentage (%)')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels([label.replace('_', '\\n') for label in LABEL_COLUMNS], rotation=45)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot 3: Training set label distribution (bar chart)\n",
    "train_label_counts = [train_df[label].sum() for label in LABEL_COLUMNS]\n",
    "axes[1, 0].bar(range(len(LABEL_COLUMNS)), train_label_counts, color='skyblue')\n",
    "axes[1, 0].set_title('Training Set Label Counts')\n",
    "axes[1, 0].set_xlabel('Labels')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_xticks(range(len(LABEL_COLUMNS)))\n",
    "axes[1, 0].set_xticklabels([label.replace('_', '\\n') for label in LABEL_COLUMNS], rotation=45)\n",
    "\n",
    "# Plot 4: Test set label distribution (bar chart)\n",
    "test_label_counts = [test_df[label].sum() for label in LABEL_COLUMNS]\n",
    "axes[1, 1].bar(range(len(LABEL_COLUMNS)), test_label_counts, color='lightcoral')\n",
    "axes[1, 1].set_title('Test Set Label Counts')\n",
    "axes[1, 1].set_xlabel('Labels')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].set_xticks(range(len(LABEL_COLUMNS)))\n",
    "axes[1, 1].set_xticklabels([label.replace('_', '\\n') for label in LABEL_COLUMNS], rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/data_split_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Update paths for the inference process\n",
    "print(f\"\\n Updating paths for inference process...\")\n",
    "TRAIN_DATA_PATH = train_split_path\n",
    "VAL_DATA_PATH = val_split_path\n",
    "TEST_DATA_PATH = test_split_no_labels_path  # Use the version without labels for prediction\n",
    "TEST_WITH_LABELS_PATH = test_split_with_labels_path  # Keep reference to version with labels\n",
    "\n",
    "print(f\"Updated paths:\")\n",
    "print(f\"- TRAIN_DATA_PATH: {TRAIN_DATA_PATH}\")\n",
    "print(f\"- VAL_DATA_PATH: {VAL_DATA_PATH}\")\n",
    "print(f\"- TEST_DATA_PATH: {TEST_DATA_PATH}\")\n",
    "print(f\"- TEST_WITH_LABELS_PATH: {TEST_WITH_LABELS_PATH}\")\n",
    "\n",
    "# Use the split data for inference\n",
    "train_df_split = train_df.copy()\n",
    "val_df_split = val_df.copy()\n",
    "test_df = test_df_no_labels.copy()  # This will be used for prediction\n",
    "\n",
    "print(f\"\\n Data splitting completed successfully!\")\n",
    "print(f\"Final dataset sizes:\")\n",
    "print(f\"- Training: {len(train_df_split):,} samples\")\n",
    "print(f\"- Validation: {len(val_df_split):,} samples\")\n",
    "print(f\"- Test: {len(test_df):,} samples\")\n",
    "print(f\"Ready to proceed with inference on the full test data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb9538",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loading Flan-T5 Model and Tokenizer\n",
    "print(\"Loading Flan-T5 model and tokenizer...\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"Tokenizer loaded. Vocab size: {len(tokenizer)}\")\n",
    "\n",
    "# Load model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded on device: {next(model.parameters()).device}\")\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
    "\n",
    "# Testing the model with a simple example\n",
    "test_prompt = \"Is this comment toxic? Answer yes or no.\\nComment: Hello world\"\n",
    "test_inputs = tokenizer(test_prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH)\n",
    "if torch.cuda.is_available():\n",
    "    test_inputs = {k: v.to(model.device) for k, v in test_inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_outputs = model.generate(\n",
    "        **test_inputs,\n",
    "        max_new_tokens=GENERATION_MAX_LENGTH,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    test_response = tokenizer.decode(test_outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Test prompt: {test_prompt}\")\n",
    "    print(f\"Test response: {test_response}\")\n",
    "\n",
    "print(\"Flan-T5 model loaded successfully and ready for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d002916e",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate Predictions using Flan-T5 Inference\n",
    "print(\"Generating predictions on test set using Flan-T5...\")\n",
    "\n",
    "# Load test data from the split\n",
    "print(f\"Loading test data from: {TEST_DATA_PATH}\")\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "print(f\"Test data loaded: {test_df.shape}\")\n",
    "\n",
    "def parse_response(response):\n",
    "    \"\"\"Parse model response to binary prediction\"\"\"\n",
    "    response = response.lower().strip()\n",
    "    if 'yes' in response:\n",
    "        return 1\n",
    "    elif 'no' in response:\n",
    "        return 0\n",
    "    else:\n",
    "        # Default to 0 if response is unclear\n",
    "        return 0\n",
    "\n",
    "def predict_toxicity_for_text(text, label_category):\n",
    "    \"\"\"Generate prediction for a single text and label category\"\"\"\n",
    "    prompt = PROMPT_TEMPLATES[label_category].format(text=text)\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=GENERATION_MAX_LENGTH,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode and parse response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return parse_response(response)\n",
    "\n",
    "# Initialize predictions array\n",
    "num_samples = len(test_df)\n",
    "num_labels = len(LABEL_COLUMNS)\n",
    "predictions = np.zeros((num_samples, num_labels), dtype=int)\n",
    "\n",
    "print(f\"Starting prediction for {num_samples} samples across {num_labels} categories...\")\n",
    "print(\"This may take some time...\")\n",
    "\n",
    "# Generate predictions for each sample and each label\n",
    "for i, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Predicting\"):\n",
    "    text = row[TEXT_COLUMN]\n",
    "    \n",
    "    # Truncate text if too long to avoid memory issues\n",
    "    if len(text) > 1000:\n",
    "        text = text[:1000] + \"...\"\n",
    "    \n",
    "    for j, label in enumerate(LABEL_COLUMNS):\n",
    "        try:\n",
    "            pred = predict_toxicity_for_text(text, label)\n",
    "            predictions[i, j] = pred\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting for sample {i}, label {label}: {e}\")\n",
    "            predictions[i, j] = 0  # Default to 0 on error\n",
    "\n",
    "print(f\"Prediction completed!\")\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"Predictions are binary: {np.all(np.isin(predictions, [0, 1]))}\")\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = test_df[['id']].copy()\n",
    "for i, label in enumerate(LABEL_COLUMNS):\n",
    "    submission_df[label] = predictions[:, i]\n",
    "\n",
    "# Save predictions\n",
    "submission_filename = f\"{OUTPUT_DIR}/test_predictions_binary.csv\"\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "print(f\"Binary test predictions saved to {submission_filename}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nSample binary test predictions:\")\n",
    "print(submission_df.head(10))\n",
    "\n",
    "# Show prediction statistics\n",
    "print(\"\\nPrediction Statistics:\")\n",
    "for i, label in enumerate(LABEL_COLUMNS):\n",
    "    positive_count = np.sum(predictions[:, i])\n",
    "    positive_percentage = (positive_count / num_samples) * 100\n",
    "    print(f\"{label.replace('_', ' ').title():<15}: {positive_count:4d} positive ({positive_percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nPrediction generation completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a4310b",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate on Split Test Set with Ground Truth Labels\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION ON SPLIT TEST SET WITH GROUND TRUTH LABELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the test set with labels for evaluation\n",
    "print(f\" Loading test set with labels from: {TEST_WITH_LABELS_PATH}\")\n",
    "test_with_labels_df = pd.read_csv(TEST_WITH_LABELS_PATH)\n",
    "print(f\" Test set with labels loaded: {test_with_labels_df.shape}\")\n",
    "\n",
    "# Load binary predictions\n",
    "predictions_file_path = f\"{OUTPUT_DIR}/test_predictions_binary.csv\"\n",
    "\n",
    "if os.path.exists(predictions_file_path):\n",
    "    print(f\" Loading binary predictions from: {predictions_file_path}\")\n",
    "    pred_df = pd.read_csv(predictions_file_path)\n",
    "    print(f\" Binary predictions loaded: {pred_df.shape}\")\n",
    "    \n",
    "    # Ensure both datasets have the same length and order\n",
    "    if len(test_with_labels_df) == len(pred_df):\n",
    "        # Extract true labels and binary predictions\n",
    "        y_true = test_with_labels_df[LABEL_COLUMNS].values.astype(int)\n",
    "        y_pred_binary = pred_df[LABEL_COLUMNS].values.astype(int)\n",
    "        \n",
    "        print(f\" Evaluation data shape: {y_true.shape}\")\n",
    "        print(f\" Predictions are binary: {np.all(np.isin(y_pred_binary, [0, 1]))}\")\n",
    "        print(f\" Labels are binary: {np.all(np.isin(y_true, [0, 1]))}\")\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        print(\"\\n SPLIT TEST SET EVALUATION RESULTS:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Per-label metrics\n",
    "        print(\"Per-Label Performance:\")\n",
    "        for i, label in enumerate(LABEL_COLUMNS):\n",
    "            true_labels = y_true[:, i]\n",
    "            pred_labels = y_pred_binary[:, i]\n",
    "            \n",
    "            # Basic metrics\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                true_labels, pred_labels, average='binary', zero_division=0\n",
    "            )\n",
    "            \n",
    "            # Accuracy for this label\n",
    "            accuracy = np.mean(true_labels == pred_labels)\n",
    "            \n",
    "            # Support\n",
    "            support = np.sum(true_labels)\n",
    "            \n",
    "            print(f\"{label.replace('_', ' ').title():<15}: P={precision:.3f} R={recall:.3f} F1={f1:.3f} Acc={accuracy:.3f} (Support: {support})\")\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        print(f\"\\n AGGREGATE PERFORMANCE:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Macro averages\n",
    "        macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred_binary, average='macro', zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Micro averages\n",
    "        micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(\n",
    "            y_true.flatten(), y_pred_binary.flatten(), average='micro', zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Exact match accuracy (all labels must be correct)\n",
    "        exact_match = np.mean(np.all(y_pred_binary == y_true, axis=1))\n",
    "        \n",
    "        # Hamming loss\n",
    "        hamming_loss = np.mean(y_pred_binary != y_true)\n",
    "        \n",
    "        # Label-wise accuracy\n",
    "        label_accuracies = [np.mean(y_true[:, i] == y_pred_binary[:, i]) for i in range(len(LABEL_COLUMNS))]\n",
    "        mean_label_accuracy = np.mean(label_accuracies)\n",
    "        \n",
    "        print(f\"Macro Average    : P={macro_precision:.3f} R={macro_recall:.3f} F1={macro_f1:.3f}\")\n",
    "        print(f\"Micro Average    : P={micro_precision:.3f} R={micro_recall:.3f} F1={micro_f1:.3f}\")\n",
    "        print(f\"Exact Match Acc  : {exact_match:.3f}\")\n",
    "        print(f\"Mean Label Acc   : {mean_label_accuracy:.3f}\")\n",
    "        print(f\"Hamming Loss     : {hamming_loss:.3f}\")\n",
    "        \n",
    "        print(f\"\\n Binary evaluation completed successfully!\")\n",
    "        print(f\" Exact Match Accuracy: {exact_match:.3f}\")\n",
    "        print(f\" Macro F1 Score: {macro_f1:.3f}\")\n",
    "        print(f\" Evaluated on {len(y_true)} test samples\")\n",
    "        \n",
    "    else:\n",
    "        print(f\" Length mismatch: Test labels ({len(test_with_labels_df)}) vs Predictions ({len(pred_df)})\")\n",
    "else:\n",
    "    print(f\" Binary predictions file not found: {predictions_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7622110,
     "sourceId": 12106625,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
