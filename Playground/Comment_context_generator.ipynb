{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69899ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Fix tokenizers warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e2785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 16225 texts for topic modeling...\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# First, let's load and prepare the data\n",
    "# Load your training data\n",
    "train_data = pd.read_csv('../Dataset/train.csv')\n",
    "\n",
    "# Get the texts\n",
    "# Filter for any type of toxic comment (toxic, severe_toxic, obscene, threat, insult, identity_hate)\n",
    "toxic_mask = (\n",
    "    (train_data['toxic'] == 1) |\n",
    "    (train_data['severe_toxic'] == 1) |\n",
    "    (train_data['obscene'] == 1) |\n",
    "    (train_data['threat'] == 1) |\n",
    "    (train_data['insult'] == 1) |\n",
    "    (train_data['identity_hate'] == 1)\n",
    ")\n",
    "train_data = train_data[toxic_mask]\n",
    "texts = train_data['comment_text'].tolist()\n",
    "\n",
    "print(f\"Processing {len(texts)} texts for topic modeling...\")\n",
    "\n",
    "# Check if we have device defined\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8d890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 16225 texts for topic modeling...\n"
     ]
    }
   ],
   "source": [
    "# Configure BERTopic for interpretable topics with max 30 clusters\n",
    "print(f\"Processing {len(texts)} texts for topic modeling...\")\n",
    "\n",
    "# Step 1: Configure embedding model (already done)\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "# Step 2: Configure UMAP for dimensionality reduction\n",
    "# Step 2: Configure UMAP for better clustering\n",
    "umap_model = UMAP(\n",
    "    n_components=15,        # Increased dimensions\n",
    "    n_neighbors=10,         # Reduced for more local structure\n",
    "    min_dist=0.0,          # Keep tight clusters\n",
    "    metric='cosine',       # Good for text embeddings\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Configure HDBSCAN to limit clusters\n",
    "# Step 3: Configure HDBSCAN with more permissive parameters\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=50,    # Reduced from 200 - smaller clusters allowed\n",
    "    min_samples=10,         # Reduced from 50 - less strict clustering\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom',\n",
    "    prediction_data=True \n",
    ")\n",
    "\n",
    "# Step 4: Configure vectorizer for better topic representation\n",
    "# Step 4: Configure vectorizer for better topic representation\n",
    "vectorizer_model = CountVectorizer(\n",
    "    ngram_range=(1, 2),     # Include bigrams for better context\n",
    "    stop_words=\"english\",   # Remove common words\n",
    "    max_features=5000,      # Limit vocabulary\n",
    "    min_df=2,               # Word must appear in at least 2 documents (reduced from 10)\n",
    "    max_df=0.95            # Remove words in >95% of documents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "092f6060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 15:31:48,362 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BERTopic model with interpretable topics...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a7848caae343d9bb31e080c6f049af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 15:32:09,824 - BERTopic - Embedding - Completed ✓\n",
      "2025-06-03 15:32:09,825 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-06-03 15:32:20,158 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-06-03 15:32:20,159 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-06-03 15:32:21,886 - BERTopic - Cluster - Completed ✓\n",
      "2025-06-03 15:32:21,886 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-06-03 15:32:23,116 - BERTopic - Representation - Completed ✓\n",
      "2025-06-03 15:32:23,120 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-06-03 15:32:23,127 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-06-03 15:32:24,400 - BERTopic - Representation - Completed ✓\n",
      "2025-06-03 15:32:24,401 - BERTopic - Topic reduction - Reduced number of topics from 3 to 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model created!\n",
      "Number of topics found: 2\n",
      "Number of outliers: 272\n"
     ]
    }
   ],
   "source": [
    "# Create and fit BERTopic model (fixed parameters)\n",
    "print(\"Creating BERTopic model with interpretable topics...\")\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    nr_topics=\"auto\",                  # Let it determine optimal number\n",
    "    calculate_probabilities=True,       # Get topic probabilities\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "topics, probs = topic_model.fit_transform(texts)\n",
    "\n",
    "print(f\"✅ Model created!\")\n",
    "print(f\"Number of topics found: {len(set(topics)) - (1 if -1 in topics else 0)}\")\n",
    "print(f\"Number of outliers: {sum(1 for t in topics if t == -1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090e46bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number of topics: 2\n",
      "Topic Information:\n",
      "   Topic  Count                                         Name  \\\n",
      "0     -1    272  -1_ass cunt_know wrote_ban don_fuck warning   \n",
      "1      0  15477                      0_gay_fat_hate hate_jew   \n",
      "2      1    476                 1_71_122_jackass jackass_156   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [ass cunt, know wrote, ban don, fuck warning, ...   \n",
      "1  [gay, fat, hate hate, jew, pig, pig pig, bulls...   \n",
      "2  [71, 122, jackass jackass, 156, 180, 76, 179, ...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [suxk my dick \\n\\nYOU FAILED AGAIN TO BLOCK ME...  \n",
      "1  [im gay\\nim gay\\nim gay\\nim gay\\nim gay\\nim ga...  \n",
      "2  [Fuck up. 122.57.32.65, Amadeus!\\nsings that A...  \n"
     ]
    }
   ],
   "source": [
    "# Get topic information and reduce if needed\n",
    "topic_info = topic_model.get_topic_info()\n",
    "current_topics = len(set(topics)) - (1 if -1 in topics else 0)\n",
    "\n",
    "print(f\"Current number of topics: {current_topics}\")\n",
    "\n",
    "if current_topics > 30:\n",
    "    print(f\"Reducing from {current_topics} to 25 topics...\")\n",
    "    topic_model.reduce_topics(texts, nr_topics=25)\n",
    "    topics = topic_model.topics_\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    print(f\"✅ Reduced to {len(set(topics)) - (1 if -1 in topics else 0)} topics\")\n",
    "\n",
    "print(\"Topic Information:\")\n",
    "print(topic_info.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d866298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number of topics: 2\n",
      "Topic Information:\n",
      "   Topic  Count                                         Name  \\\n",
      "0     -1    272  -1_ass cunt_know wrote_ban don_fuck warning   \n",
      "1      0  15477                      0_gay_fat_hate hate_jew   \n",
      "2      1    476                 1_71_122_jackass jackass_156   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [ass cunt, know wrote, ban don, fuck warning, ...   \n",
      "1  [gay, fat, hate hate, jew, pig, pig pig, bulls...   \n",
      "2  [71, 122, jackass jackass, 156, 180, 76, 179, ...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [suxk my dick \\n\\nYOU FAILED AGAIN TO BLOCK ME...  \n",
      "1  [im gay\\nim gay\\nim gay\\nim gay\\nim gay\\nim ga...  \n",
      "2  [Fuck up. 122.57.32.65, Amadeus!\\nsings that A...  \n"
     ]
    }
   ],
   "source": [
    "# Get topic information and reduce if needed\n",
    "topic_info = topic_model.get_topic_info()\n",
    "current_topics = len(set(topics)) - (1 if -1 in topics else 0)\n",
    "\n",
    "print(f\"Current number of topics: {current_topics}\")\n",
    "\n",
    "if current_topics > 30:\n",
    "    print(f\"Reducing from {current_topics} to 25 topics...\")\n",
    "    topic_model.reduce_topics(texts, nr_topics=25)\n",
    "    topics = topic_model.topics_\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    print(f\"✅ Reduced to {len(set(topics)) - (1 if -1 in topics else 0)} topics\")\n",
    "\n",
    "print(\"Topic Information:\")\n",
    "print(topic_info.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c38ac3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number of topics: 2\n",
      "Topic Information:\n",
      "   Topic  Count                                         Name  \\\n",
      "0     -1    272  -1_ass cunt_know wrote_ban don_fuck warning   \n",
      "1      0  15477                      0_gay_fat_hate hate_jew   \n",
      "2      1    476                 1_71_122_jackass jackass_156   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [ass cunt, know wrote, ban don, fuck warning, ...   \n",
      "1  [gay, fat, hate hate, jew, pig, pig pig, bulls...   \n",
      "2  [71, 122, jackass jackass, 156, 180, 76, 179, ...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [suxk my dick \\n\\nYOU FAILED AGAIN TO BLOCK ME...  \n",
      "1  [im gay\\nim gay\\nim gay\\nim gay\\nim gay\\nim ga...  \n",
      "2  [Fuck up. 122.57.32.65, Amadeus!\\nsings that A...  \n"
     ]
    }
   ],
   "source": [
    "# Get topic information and reduce if needed\n",
    "topic_info = topic_model.get_topic_info()\n",
    "current_topics = len(set(topics)) - (1 if -1 in topics else 0)\n",
    "\n",
    "print(f\"Current number of topics: {current_topics}\")\n",
    "\n",
    "if current_topics > 30:\n",
    "    print(f\"Reducing from {current_topics} to 25 topics...\")\n",
    "    topic_model.reduce_topics(texts, nr_topics=25)\n",
    "    topics = topic_model.topics_\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    print(f\"✅ Reduced to {len(set(topics)) - (1 if -1 in topics else 0)} topics\")\n",
    "\n",
    "print(\"Topic Information:\")\n",
    "print(topic_info.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "299a1a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Names:\n",
      "Topic 0: gay | fat | hate hate (Count: 15477)\n",
      "Topic 1: 71 | 122 | jackass jackass (Count: 476)\n"
     ]
    }
   ],
   "source": [
    "# Create interpretable topic names using top words\n",
    "topic_names = {}\n",
    "for topic_id in topic_info['Topic'].unique():\n",
    "    if topic_id == -1:\n",
    "        topic_names[topic_id] = \"Outliers\"\n",
    "        continue\n",
    "    \n",
    "    # Get top 3 words for this topic\n",
    "    topic_words = topic_model.get_topic(topic_id)\n",
    "    if topic_words:\n",
    "        # Extract top 3 words\n",
    "        top_words = [word for word, score in topic_words[:3]]\n",
    "        topic_names[topic_id] = \" | \".join(top_words)\n",
    "    else:\n",
    "        topic_names[topic_id] = f\"Topic_{topic_id}\"\n",
    "\n",
    "# Show topic names\n",
    "print(\"\\nTopic Names:\")\n",
    "for topic_id, name in topic_names.items():\n",
    "    if topic_id != -1:\n",
    "        count = topic_info[topic_info['Topic'] == topic_id]['Count'].iloc[0]\n",
    "        print(f\"Topic {topic_id}: {name} (Count: {count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "262e5729",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (16225) does not match length of index (159571)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m interpretable_topic_labels \u001b[38;5;241m=\u001b[39m [topic_names\u001b[38;5;241m.\u001b[39mget(topic, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m topics]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Add to training data\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtopic_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m topics\n\u001b[1;32m      6\u001b[0m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m interpretable_topic_labels\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Show distribution\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4517\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4530\u001b[0m     ):\n\u001b[1;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5266\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[1;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[1;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (16225) does not match length of index (159571)"
     ]
    }
   ],
   "source": [
    "# Apply topic names to your data\n",
    "interpretable_topic_labels = [topic_names.get(topic, \"Unknown\") for topic in topics]\n",
    "\n",
    "# Add to training data\n",
    "train_data['topic_id'] = topics\n",
    "train_data['topic_name'] = interpretable_topic_labels\n",
    "\n",
    "# Show distribution\n",
    "from collections import Counter\n",
    "topic_distribution = Counter(interpretable_topic_labels)\n",
    "\n",
    "print(f\"\\nFinal Topic Distribution:\")\n",
    "for topic, count in topic_distribution.most_common(20):\n",
    "    print(f\"  {topic}: {count} ({count/len(texts)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e78c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5475d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample texts for top topics\n",
    "print(f\"\\n📝 Sample texts for top 5 topics:\")\n",
    "for topic_name, count in topic_distribution.most_common(5):\n",
    "    print(f\"\\n--- {topic_name} ({count} texts) ---\")\n",
    "    sample_indices = train_data[train_data['topic_name'] == topic_name].index[:3]\n",
    "    for i, idx in enumerate(sample_indices, 1):\n",
    "        text = train_data.loc[idx, 'comment_text']\n",
    "        display_text = text[:120] + \"...\" if len(text) > 120 else text\n",
    "        print(f\"{i}. {display_text}\")\n",
    "\n",
    "# Save results\n",
    "output_path = '../Dataset/train_with_bertopic_final.csv'\n",
    "train_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ COMPLETED!\")\n",
    "print(f\"📁 Dataset saved: {output_path}\")\n",
    "print(f\"📊 Total topics: {len(set(topics)) - (1 if -1 in topics else 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3815e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Group texts by cluster label (excluding noise, label == -1)\n",
    "cluster_examples = defaultdict(list)\n",
    "for text, label in zip(texts, cluster_labels):\n",
    "    if label != -1:\n",
    "        cluster_examples[label].append(text)\n",
    "\n",
    "# Show 5 random examples from each cluster (up to 5 if less)\n",
    "for label, examples in cluster_examples.items():\n",
    "    print(f\"\\nCluster {label} (keywords: {cluster_keywords.get(label, '')}):\")\n",
    "    for example in random.sample(examples, min(5, len(examples))):\n",
    "        print(f\"- {example[:200]}{'...' if len(example) > 200 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef86cba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_top_keywords(texts, top_n=3):\n",
    "    tfidf = TfidfVectorizer(stop_words='english', max_features=10)\n",
    "    tfidf_matrix = tfidf.fit_transform(texts)\n",
    "    summed = tfidf_matrix.sum(axis=0).A1\n",
    "    keywords = np.array(tfidf.get_feature_names_out())[np.argsort(summed)[::-1][:top_n]]\n",
    "    return \", \".join(keywords)\n",
    "\n",
    "# Map clusters to keywords\n",
    "cluster_keywords = {}\n",
    "for label in set(cluster_labels):\n",
    "    if label == -1:\n",
    "        continue  # Skip noise\n",
    "    cluster_texts = [t for t, l in zip(texts, cluster_labels) if l == label]\n",
    "    cluster_keywords[label] = get_top_keywords(cluster_texts)\n",
    "\n",
    "# Assign keywords as labels\n",
    "interpretable_labels = [\n",
    "    cluster_keywords.get(lbl, \"Noise\") for lbl in cluster_labels\n",
    "]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
