%!TEX root = ../hbrs-poster.tex


\block{Abstract}
{
    With the explosion of user-generated content online, ensuring safe and respectful digital spaces has become more important than ever. In this project, we address the challenge of multi-label toxic comment classification using a modern NLP pipeline. Our approach begins with thorough text cleaning and tokenization using Ekphrasis, tailored for the quirks of social media language. We extract features through a range of techniques, including TF-IDF, Word2Vec, GloVe, and Sentence-Transformer MiniLM embeddings. For modeling, we explore both classical algorithms (Logistic Regression, LightGBM, XGBoost), deep learning architectures (BiGRU, BiLSTM, Attention), and fine-tuned transformer models (BERT, RoBERTa, GPT-1, FLAN-T5). To tackle class imbalance and boost performance, we incorporate adaptive focal loss, oversampling, and threshold tuning. Our results highlight that transformer-based models, especially with advanced fine-tuning, excel at detecting nuanced toxic language. This demonstrates the real-world impact of recent NLP advances for content moderation.
}